{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpleCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOIoyL048iIm688//2pmZlG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc6288a5d2f3429299c15ca08fbdb44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a43cfda16702446187f68dd9458fb12b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d292e5acdac40e49b195fbdd0025d16",
              "IPY_MODEL_890798579b8043fa9f08629a93258d82",
              "IPY_MODEL_5789cd2e3cb7491fa39642bf039ba560"
            ]
          }
        },
        "a43cfda16702446187f68dd9458fb12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d292e5acdac40e49b195fbdd0025d16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_972175a10e6947c0a5cb0488dccaea91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3085a25953ee43bfabb764314a4d1371"
          }
        },
        "890798579b8043fa9f08629a93258d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c515080f89a84ee881acde516dafce3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 241627721,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 241627721,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4e961e06e4a416d864a2fe602b13612"
          }
        },
        "5789cd2e3cb7491fa39642bf039ba560": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81f792c958fe4ffab4589fb4ade0b170",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 230M/230M [00:02&lt;00:00, 87.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_21184b8338704d3e8be660f4dedcf220"
          }
        },
        "972175a10e6947c0a5cb0488dccaea91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3085a25953ee43bfabb764314a4d1371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c515080f89a84ee881acde516dafce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4e961e06e4a416d864a2fe602b13612": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81f792c958fe4ffab4589fb4ade0b170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "21184b8338704d3e8be660f4dedcf220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junhyeokk/boomhill24/blob/main/model/baseline-model-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HcFFgpBmMEd",
        "outputId": "6a3ee329-3bba-4053-d7a1-af4e309858f8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Gs6Oi7pnkM"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image as PIL_Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, resnet152"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcEXE1AYpsHp"
      },
      "source": [
        "seed = 400\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)    \n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDbBQ6uNrBnS"
      },
      "source": [
        "drivepath = \"/content/gdrive/MyDrive\"\n",
        "shared_drivepath = \"/content/gdrive/Shareddrives/umapyoi\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHRZVnHfrEwY",
        "outputId": "e2cd70cc-ce79-490e-c8ea-ae0f58d72550"
      },
      "source": [
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device_type = \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55PzuQKBrHEs"
      },
      "source": [
        "## 모델 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URMGdLjzrJVk"
      },
      "source": [
        "class KartModel1(nn.Module):\n",
        "  def __init__(self, class_num = 8):\n",
        "    super(KartModel1, self).__init__()\n",
        "    self.class_num = class_num\n",
        "    self.backbone = resnet152(pretrained=True)\n",
        "    in_features_num = self.backbone.fc.in_features\n",
        "    \n",
        "    self.backbone.fc = nn.Sequential(\n",
        "      nn.Linear(in_features=in_features_num, out_features=256, bias=True),\n",
        "      nn.BatchNorm1d(256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(in_features=256, out_features=class_num, bias=True),\n",
        "      # nn.Softmax(dim=1)\n",
        "      # nn.Sigmoid(),\n",
        "    )\n",
        "  \n",
        "  def forward(self, input_image):\n",
        "    output = self.backbone(input_image)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9t_aTRr-xb"
      },
      "source": [
        "class KartDataSet1(data.Dataset):\n",
        "  def __init__(self, csv_files):\n",
        "    self.images = []\n",
        "    self.labels = []\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "      with open(csv_file) as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        # next(csv_reader, None)        # 첫번째 row 스킵\n",
        "        \n",
        "        image_path = '/' + '/'.join(csv_file.split('/')[:-1]) + \"/img/\"\n",
        "        for row in csv_reader:\n",
        "          self.images.append(image_path + row[0])\n",
        "          # self.labels.append([int(x) for x in list(row[1][:3])])\n",
        "          self.labels.append(int(row[1][:3], 2))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.images[index]\n",
        "    image = PIL_Image.open(image_path)\n",
        "\n",
        "    label = self.labels[index]\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),     # -1 ~ 1 로 normalize\n",
        "    ])\n",
        "    \n",
        "    return preprocess(image), label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrQcjHAWwEjk",
        "outputId": "aa8811af-0208-4c5b-eb10-55b5b9b07002"
      },
      "source": [
        "train_csvs = []\n",
        "for csv_file in glob.glob(f\"{shared_drivepath}/simpleCNNdata/**/*.csv\"):\n",
        "  train_csvs.append(csv_file)\n",
        "print(len(train_csvs))\n",
        "print(train_csvs[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n",
            "/content/gdrive/Shareddrives/umapyoi/simpleCNNdata/ice_penguin/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76LZ-il72ZKn",
        "outputId": "02fea00e-4187-4c27-e976-f90233e9c165"
      },
      "source": [
        "train_dataset = KartDataSet1(train_csvs)\n",
        "print(len(train_dataset))\n",
        "print(train_dataset[0][0].shape)\n",
        "print(train_dataset[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58683\n",
            "torch.Size([3, 224, 224])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S84A9U53Xnq"
      },
      "source": [
        "num_epochs = 5\n",
        "lr = 1e-4\n",
        "batch_size = 32\n",
        "log_interval = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tStDb_P34ySI",
        "outputId": "cfab8f55-d89c-4545-cf22-0febacd11b80"
      },
      "source": [
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = batch_size,\n",
        "    num_workers = 4,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "print(len(train_loader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bc6288a5d2f3429299c15ca08fbdb44e",
            "a43cfda16702446187f68dd9458fb12b",
            "3d292e5acdac40e49b195fbdd0025d16",
            "890798579b8043fa9f08629a93258d82",
            "5789cd2e3cb7491fa39642bf039ba560",
            "972175a10e6947c0a5cb0488dccaea91",
            "3085a25953ee43bfabb764314a4d1371",
            "c515080f89a84ee881acde516dafce3d",
            "d4e961e06e4a416d864a2fe602b13612",
            "81f792c958fe4ffab4589fb4ade0b170",
            "21184b8338704d3e8be660f4dedcf220"
          ]
        },
        "id": "sn_CeRo847zk",
        "outputId": "dbab4233-c3be-4076-d9e9-7c73f79dc185"
      },
      "source": [
        "model = KartModel1()\n",
        "\n",
        "for param, weight in model.named_parameters():\n",
        "  print(f\"param {param:20} required gradient? -> {weight.requires_grad}\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc6288a5d2f3429299c15ca08fbdb44e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/230M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "param backbone.conv1.weight required gradient? -> True\n",
            "param backbone.bn1.weight  required gradient? -> True\n",
            "param backbone.bn1.bias    required gradient? -> True\n",
            "param backbone.layer1.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer1.0.conv3.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn3.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn3.bias required gradient? -> True\n",
            "param backbone.layer1.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer1.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer1.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer1.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer1.1.conv3.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn3.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn3.bias required gradient? -> True\n",
            "param backbone.layer1.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer1.2.conv3.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn3.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.0.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer2.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer2.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer2.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.1.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.2.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.3.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.3.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.3.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.4.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.4.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.4.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.4.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.5.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.5.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.5.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.5.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.6.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.6.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.6.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.6.bn3.bias required gradient? -> True\n",
            "param backbone.layer2.7.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.7.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.7.conv3.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn3.weight required gradient? -> True\n",
            "param backbone.layer2.7.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.0.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer3.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer3.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer3.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.1.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.2.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.3.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.3.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.3.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.4.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.4.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.4.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.5.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.5.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.5.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.6.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.6.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.6.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.6.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.7.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.7.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.7.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.7.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.8.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.8.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.8.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.8.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.9.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.9.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.9.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.9.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.10.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.10.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.10.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.10.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.11.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.11.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.11.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.11.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.12.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.12.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.12.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.12.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.13.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.13.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.13.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.13.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.14.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.14.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.14.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.14.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.15.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.15.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.15.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.15.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.16.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.16.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.16.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.16.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.17.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.17.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.17.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.17.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.18.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.18.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.18.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.18.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.19.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.19.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.19.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.19.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.20.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.20.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.20.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.20.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.21.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.21.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.21.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.21.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.22.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.22.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.22.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.22.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.23.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.23.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.23.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.23.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.24.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.24.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.24.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.24.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.25.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.25.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.25.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.25.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.26.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.26.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.26.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.26.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.27.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.27.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.27.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.27.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.28.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.28.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.28.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.28.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.29.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.29.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.29.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.29.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.30.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.30.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.30.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.30.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.31.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.31.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.31.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.31.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.32.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.32.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.32.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.32.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.33.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.33.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.33.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.33.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.34.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.34.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.34.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.34.bn3.bias required gradient? -> True\n",
            "param backbone.layer3.35.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.35.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.35.conv3.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn3.weight required gradient? -> True\n",
            "param backbone.layer3.35.bn3.bias required gradient? -> True\n",
            "param backbone.layer4.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.0.conv3.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn3.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn3.bias required gradient? -> True\n",
            "param backbone.layer4.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer4.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer4.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer4.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.1.conv3.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn3.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn3.bias required gradient? -> True\n",
            "param backbone.layer4.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.2.conv3.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn3.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn3.bias required gradient? -> True\n",
            "param backbone.fc.0.weight required gradient? -> True\n",
            "param backbone.fc.0.bias   required gradient? -> True\n",
            "param backbone.fc.1.weight required gradient? -> True\n",
            "param backbone.fc.1.bias   required gradient? -> True\n",
            "param backbone.fc.3.weight required gradient? -> True\n",
            "param backbone.fc.3.bias   required gradient? -> True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-fQnhla5Nz8"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "s8pu5HcO5dYX",
        "outputId": "3edbdb94-0c2b-41be-9513-92614ee517c4"
      },
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from IPython.display import Image\n",
        "to_img = ToPILImage()\n",
        "\n",
        "it = iter(train_loader)\n",
        "inputs, labels = next(it)\n",
        "display(to_img(inputs[0]))\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.to(device)\n",
        "outs = model(inputs)\n",
        "preds = torch.argmax(outs, dim=-1)\n",
        "print(preds)\n",
        "print(labels)\n",
        "print((preds == labels).sum())\n",
        "\n",
        "loss = criterion(outs, labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7F67E77859D0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4, 7, 7, 1, 7, 3, 7, 7, 1, 1, 1, 0, 7, 1, 1, 1, 1, 7, 1, 1, 7, 4, 7, 2,\n",
            "        1, 7, 1, 0, 1, 1, 1, 7], device='cuda:0')\n",
            "tensor([6, 4, 4, 5, 4, 4, 4, 6, 6, 4, 4, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        4, 4, 4, 4, 4, 1, 4, 5], device='cuda:0')\n",
            "tensor(2, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1RQI_IF57D0",
        "outputId": "f7f8a0c5-d3b8-4fb6-a849-881dec2bda96"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 53.1 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=f55589500d3a8f43e108166f7bad910eb7ee452fda954396118e186006d63791\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=eda5358e48448e9bb8ac8c1311a5e42cf2d43bbdedf88dd473cfbaa606f04782\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.2 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "bv_haTDc6cEQ",
        "outputId": "201feef2-33b0-401b-d291-2f9911347b62"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "nhxetsAa6hhJ",
        "outputId": "d788bbe5-1648-4cfb-9731-32a1b420aabd"
      },
      "source": [
        "config = {}\n",
        "config['n_epochs'] = num_epochs\n",
        "config['batch_size'] = batch_size\n",
        "config['lr'] = lr\n",
        "\n",
        "wandb.init(project=\"boomhill24_14\", config=config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunhyeokk\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.2<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">sleek-rain-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/junhyeokk/boomhill24_14\" target=\"_blank\">https://wandb.ai/junhyeokk/boomhill24_14</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/junhyeokk/boomhill24_14/runs/3dmuy1fy\" target=\"_blank\">https://wandb.ai/junhyeokk/boomhill24_14/runs/3dmuy1fy</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210920_115420-3dmuy1fy</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f65fd487910>"
            ],
            "text/html": [
              "<h1>Run(3dmuy1fy)</h1><iframe src=\"https://wandb.ai/junhyeokk/boomhill24_14/runs/3dmuy1fy\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqOJ2DUc6pSv",
        "outputId": "13434bd3-4d7f-4dec-fb3e-58ae022dd07d"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  loss_value = 0\n",
        "  matches = 0\n",
        "\n",
        "  for idx, train_batch in enumerate(train_loader):\n",
        "    inputs, labels = train_batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outs = model(inputs)\n",
        "    preds = torch.argmax(outs, dim = -1)\n",
        "    loss = criterion(outs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_value += loss.item()\n",
        "    matches += (preds == labels).sum().item()\n",
        "\n",
        "    if (idx + 1) % log_interval == 0:\n",
        "      train_loss = loss_value / batch_size / log_interval\n",
        "      train_acc = matches / batch_size / log_interval\n",
        "      current_lr = scheduler.get_last_lr()\n",
        "\n",
        "      wandb.log({\"epoch\" : epoch, \"training_loss\" : train_loss, \"training_acc\" : train_acc, \"lr\" : current_lr})\n",
        "      print(\n",
        "          f\"Epoch[{epoch + 1}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \",\n",
        "          f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
        "      )\n",
        "\n",
        "      loss_value = 0\n",
        "      matches = 0\n",
        "\n",
        "    scheduler.step()\n",
        "  \n",
        "  torch.save(model.state_dict(), f\"{shared_drivepath}/simpleCNN_ep{epoch}.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1/5](20/1834) ||  training loss 0.06026 || training accuracy 40.31% || lr [6.840622763423389e-05]\n",
            "Epoch[1/5](40/1834) ||  training loss 0.05168 || training accuracy 58.75% || lr [1.1474337861210536e-05]\n",
            "Epoch[1/5](60/1834) ||  training loss 0.04708 || training accuracy 64.69% || lr [7.78360372489924e-06]\n",
            "Epoch[1/5](80/1834) ||  training loss 0.04707 || training accuracy 64.06% || lr [6.243449435824273e-05]\n",
            "Epoch[1/5](100/1834) ||  training loss 0.04114 || training accuracy 68.12% || lr [9.990133642141353e-05]\n",
            "Epoch[1/5](120/1834) ||  training loss 0.0344 || training accuracy 72.03% || lr [6.840622763423391e-05]\n",
            "Epoch[1/5](140/1834) ||  training loss 0.03247 || training accuracy 73.59% || lr [1.147433786121053e-05]\n",
            "Epoch[1/5](160/1834) ||  training loss 0.03433 || training accuracy 72.03% || lr [7.783603724899249e-06]\n",
            "Epoch[1/5](180/1834) ||  training loss 0.03125 || training accuracy 74.38% || lr [6.243449435824274e-05]\n",
            "Epoch[1/5](200/1834) ||  training loss 0.03131 || training accuracy 70.00% || lr [9.990133642141361e-05]\n",
            "Epoch[1/5](220/1834) ||  training loss 0.03014 || training accuracy 72.50% || lr [6.840622763423395e-05]\n",
            "Epoch[1/5](240/1834) ||  training loss 0.02977 || training accuracy 72.03% || lr [1.1474337861210613e-05]\n",
            "Epoch[1/5](260/1834) ||  training loss 0.02871 || training accuracy 70.62% || lr [7.783603724899242e-06]\n",
            "Epoch[1/5](280/1834) ||  training loss 0.02727 || training accuracy 73.75% || lr [6.243449435824276e-05]\n",
            "Epoch[1/5](300/1834) ||  training loss 0.02851 || training accuracy 73.75% || lr [9.99013364214137e-05]\n",
            "Epoch[1/5](320/1834) ||  training loss 0.02893 || training accuracy 70.00% || lr [6.8406227634234e-05]\n",
            "Epoch[1/5](340/1834) ||  training loss 0.02784 || training accuracy 69.22% || lr [1.1474337861210679e-05]\n",
            "Epoch[1/5](360/1834) ||  training loss 0.02786 || training accuracy 71.09% || lr [7.783603724899573e-06]\n",
            "Epoch[1/5](380/1834) ||  training loss 0.02548 || training accuracy 73.28% || lr [6.243449435824621e-05]\n",
            "Epoch[1/5](400/1834) ||  training loss 0.02644 || training accuracy 69.53% || lr [9.99013364214192e-05]\n",
            "Epoch[1/5](420/1834) ||  training loss 0.0222 || training accuracy 77.34% || lr [6.840622763423778e-05]\n",
            "Epoch[1/5](440/1834) ||  training loss 0.026 || training accuracy 71.09% || lr [1.1474337861211202e-05]\n",
            "Epoch[1/5](460/1834) ||  training loss 0.02502 || training accuracy 70.16% || lr [7.783603724898788e-06]\n",
            "Epoch[1/5](480/1834) ||  training loss 0.02419 || training accuracy 74.22% || lr [6.243449435823923e-05]\n",
            "Epoch[1/5](500/1834) ||  training loss 0.02264 || training accuracy 74.38% || lr [9.990133642140804e-05]\n",
            "Epoch[1/5](520/1834) ||  training loss 0.0225 || training accuracy 77.34% || lr [6.840622763423e-05]\n",
            "Epoch[1/5](540/1834) ||  training loss 0.02378 || training accuracy 74.69% || lr [1.1474337861209815e-05]\n",
            "Epoch[1/5](560/1834) ||  training loss 0.02511 || training accuracy 71.88% || lr [7.78360372489975e-06]\n",
            "Epoch[1/5](580/1834) ||  training loss 0.02354 || training accuracy 74.69% || lr [6.243449435824633e-05]\n",
            "Epoch[1/5](600/1834) ||  training loss 0.02254 || training accuracy 74.38% || lr [9.99013364214191e-05]\n",
            "Epoch[1/5](620/1834) ||  training loss 0.02472 || training accuracy 72.34% || lr [6.840622763423756e-05]\n",
            "Epoch[1/5](640/1834) ||  training loss 0.02265 || training accuracy 73.59% || lr [1.1474337861211324e-05]\n",
            "Epoch[1/5](660/1834) ||  training loss 0.02171 || training accuracy 74.22% || lr [7.783603724899557e-06]\n",
            "Epoch[1/5](680/1834) ||  training loss 0.02133 || training accuracy 74.38% || lr [6.243449435824643e-05]\n",
            "Epoch[1/5](700/1834) ||  training loss 0.02085 || training accuracy 77.19% || lr [9.990133642141932e-05]\n",
            "Epoch[1/5](720/1834) ||  training loss 0.02383 || training accuracy 70.31% || lr [6.840622763423774e-05]\n",
            "Epoch[1/5](740/1834) ||  training loss 0.0225 || training accuracy 71.41% || lr [1.1474337861211365e-05]\n",
            "Epoch[1/5](760/1834) ||  training loss 0.02181 || training accuracy 73.28% || lr [7.78360372489974e-06]\n",
            "Epoch[1/5](780/1834) ||  training loss 0.02274 || training accuracy 71.88% || lr [6.2434494358246e-05]\n",
            "Epoch[1/5](800/1834) ||  training loss 0.02336 || training accuracy 73.28% || lr [9.990133642141913e-05]\n",
            "Epoch[1/5](820/1834) ||  training loss 0.02174 || training accuracy 75.00% || lr [6.84062276342376e-05]\n",
            "Epoch[1/5](840/1834) ||  training loss 0.01973 || training accuracy 76.09% || lr [1.1474337861211341e-05]\n",
            "Epoch[1/5](860/1834) ||  training loss 0.02035 || training accuracy 75.78% || lr [7.783603724899735e-06]\n",
            "Epoch[1/5](880/1834) ||  training loss 0.01965 || training accuracy 74.06% || lr [6.243449435824601e-05]\n",
            "Epoch[1/5](900/1834) ||  training loss 0.02103 || training accuracy 74.22% || lr [9.990133642141925e-05]\n",
            "Epoch[1/5](920/1834) ||  training loss 0.02177 || training accuracy 76.25% || lr [6.840622763423803e-05]\n",
            "Epoch[1/5](940/1834) ||  training loss 0.0208 || training accuracy 76.72% || lr [1.1474337861211136e-05]\n",
            "Epoch[1/5](960/1834) ||  training loss 0.01956 || training accuracy 76.09% || lr [7.783603724899972e-06]\n",
            "Epoch[1/5](980/1834) ||  training loss 0.02027 || training accuracy 78.28% || lr [6.24344943582498e-05]\n",
            "Epoch[1/5](1000/1834) ||  training loss 0.02041 || training accuracy 77.03% || lr [9.990133642142477e-05]\n",
            "Epoch[1/5](1020/1834) ||  training loss 0.02018 || training accuracy 76.56% || lr [6.840622763424149e-05]\n",
            "Epoch[1/5](1040/1834) ||  training loss 0.01932 || training accuracy 76.88% || lr [1.1474337861212005e-05]\n",
            "Epoch[1/5](1060/1834) ||  training loss 0.02001 || training accuracy 75.78% || lr [7.783603724901909e-06]\n",
            "Epoch[1/5](1080/1834) ||  training loss 0.01807 || training accuracy 79.06% || lr [6.243449435826355e-05]\n",
            "Epoch[1/5](1100/1834) ||  training loss 0.0196 || training accuracy 76.41% || lr [9.990133642144732e-05]\n",
            "Epoch[1/5](1120/1834) ||  training loss 0.02037 || training accuracy 73.59% || lr [6.840622763425726e-05]\n",
            "Epoch[1/5](1140/1834) ||  training loss 0.02085 || training accuracy 76.09% || lr [1.1474337861214826e-05]\n",
            "Epoch[1/5](1160/1834) ||  training loss 0.01925 || training accuracy 75.94% || lr [7.78360372489996e-06]\n",
            "Epoch[1/5](1180/1834) ||  training loss 0.01892 || training accuracy 75.62% || lr [6.243449435824913e-05]\n",
            "Epoch[1/5](1200/1834) ||  training loss 0.01964 || training accuracy 76.88% || lr [9.990133642142483e-05]\n",
            "Epoch[1/5](1220/1834) ||  training loss 0.01901 || training accuracy 78.44% || lr [6.840622763424156e-05]\n",
            "Epoch[1/5](1240/1834) ||  training loss 0.01868 || training accuracy 76.88% || lr [1.1474337861212027e-05]\n",
            "Epoch[1/5](1260/1834) ||  training loss 0.02168 || training accuracy 71.56% || lr [7.783603724901897e-06]\n",
            "Epoch[1/5](1280/1834) ||  training loss 0.01787 || training accuracy 79.06% || lr [6.243449435826353e-05]\n",
            "Epoch[1/5](1300/1834) ||  training loss 0.01951 || training accuracy 76.56% || lr [9.990133642144733e-05]\n",
            "Epoch[1/5](1320/1834) ||  training loss 0.0207 || training accuracy 74.22% || lr [6.84062276342573e-05]\n",
            "Epoch[1/5](1340/1834) ||  training loss 0.01834 || training accuracy 78.75% || lr [1.1474337861214399e-05]\n",
            "Epoch[1/5](1360/1834) ||  training loss 0.01918 || training accuracy 75.31% || lr [7.783603724899945e-06]\n",
            "Epoch[1/5](1380/1834) ||  training loss 0.01909 || training accuracy 74.53% || lr [6.243449435824975e-05]\n",
            "Epoch[1/5](1400/1834) ||  training loss 0.0182 || training accuracy 77.66% || lr [9.990133642142478e-05]\n",
            "Epoch[1/5](1420/1834) ||  training loss 0.02 || training accuracy 75.00% || lr [6.840622763424153e-05]\n",
            "Epoch[1/5](1440/1834) ||  training loss 0.01809 || training accuracy 78.59% || lr [1.1474337861212037e-05]\n",
            "Epoch[1/5](1460/1834) ||  training loss 0.01989 || training accuracy 75.62% || lr [7.783603724898382e-06]\n",
            "Epoch[1/5](1480/1834) ||  training loss 0.02051 || training accuracy 74.84% || lr [6.243449435823605e-05]\n",
            "Epoch[1/5](1500/1834) ||  training loss 0.02002 || training accuracy 75.31% || lr [9.990133642140224e-05]\n",
            "Epoch[1/5](1520/1834) ||  training loss 0.01849 || training accuracy 77.34% || lr [6.840622763422649e-05]\n",
            "Epoch[1/5](1540/1834) ||  training loss 0.02122 || training accuracy 74.53% || lr [1.1474337861209229e-05]\n",
            "Epoch[1/5](1560/1834) ||  training loss 0.02036 || training accuracy 75.94% || lr [7.783603724896815e-06]\n",
            "Epoch[1/5](1580/1834) ||  training loss 0.01959 || training accuracy 73.91% || lr [6.2434494358221e-05]\n",
            "Epoch[1/5](1600/1834) ||  training loss 0.01855 || training accuracy 75.78% || lr [9.990133642137988e-05]\n",
            "Epoch[1/5](1620/1834) ||  training loss 0.01778 || training accuracy 77.50% || lr [6.84062276342108e-05]\n",
            "Epoch[1/5](1640/1834) ||  training loss 0.02014 || training accuracy 77.03% || lr [1.1474337861206444e-05]\n",
            "Epoch[1/5](1660/1834) ||  training loss 0.01868 || training accuracy 76.56% || lr [7.783603724901929e-06]\n",
            "Epoch[1/5](1680/1834) ||  training loss 0.01787 || training accuracy 79.22% || lr [6.243449435826697e-05]\n",
            "Epoch[1/5](1700/1834) ||  training loss 0.02012 || training accuracy 77.19% || lr [9.990133642145292e-05]\n",
            "Epoch[1/5](1720/1834) ||  training loss 0.01723 || training accuracy 78.75% || lr [6.840622763426048e-05]\n",
            "Epoch[1/5](1740/1834) ||  training loss 0.01675 || training accuracy 80.47% || lr [1.1474337861215519e-05]\n",
            "Epoch[1/5](1760/1834) ||  training loss 0.01935 || training accuracy 77.50% || lr [7.783603724900359e-06]\n",
            "Epoch[1/5](1780/1834) ||  training loss 0.0189 || training accuracy 77.50% || lr [6.243449435825326e-05]\n",
            "Epoch[1/5](1800/1834) ||  training loss 0.01983 || training accuracy 75.31% || lr [9.990133642143045e-05]\n",
            "Epoch[1/5](1820/1834) ||  training loss 0.01777 || training accuracy 79.38% || lr [6.840622763424614e-05]\n",
            "Epoch[2/5](20/1834) ||  training loss 0.01647 || training accuracy 80.16% || lr [8.85637463565464e-07]\n",
            "Epoch[2/5](40/1834) ||  training loss 0.01931 || training accuracy 77.03% || lr [4.373333832178238e-05]\n",
            "Epoch[2/5](60/1834) ||  training loss 0.01553 || training accuracy 80.94% || lr [9.524135262329545e-05]\n",
            "Epoch[2/5](80/1834) ||  training loss 0.01801 || training accuracy 77.34% || lr [8.422735529642963e-05]\n",
            "Epoch[2/5](100/1834) ||  training loss 0.01944 || training accuracy 75.31% || lr [2.5912316294913023e-05]\n",
            "Epoch[2/5](120/1834) ||  training loss 0.01721 || training accuracy 78.28% || lr [8.856374635657245e-07]\n",
            "Epoch[2/5](140/1834) ||  training loss 0.01823 || training accuracy 73.75% || lr [4.373333832179187e-05]\n",
            "Epoch[2/5](160/1834) ||  training loss 0.01977 || training accuracy 75.16% || lr [9.52413526233171e-05]\n",
            "Epoch[2/5](180/1834) ||  training loss 0.0192 || training accuracy 75.62% || lr [8.422735529644884e-05]\n",
            "Epoch[2/5](200/1834) ||  training loss 0.01894 || training accuracy 76.41% || lr [2.5912316294919172e-05]\n",
            "Epoch[2/5](220/1834) ||  training loss 0.01717 || training accuracy 77.03% || lr [8.856374635658572e-07]\n",
            "Epoch[2/5](240/1834) ||  training loss 0.01918 || training accuracy 76.72% || lr [4.373333832180204e-05]\n",
            "Epoch[2/5](260/1834) ||  training loss 0.01572 || training accuracy 80.47% || lr [9.524135262333861e-05]\n",
            "Epoch[2/5](280/1834) ||  training loss 0.0186 || training accuracy 76.41% || lr [8.422735529646807e-05]\n",
            "Epoch[2/5](300/1834) ||  training loss 0.01681 || training accuracy 79.22% || lr [2.5912316294924702e-05]\n",
            "Epoch[2/5](320/1834) ||  training loss 0.01868 || training accuracy 76.88% || lr [8.856374635657245e-07]\n",
            "Epoch[2/5](340/1834) ||  training loss 0.01678 || training accuracy 79.38% || lr [4.373333832179255e-05]\n",
            "Epoch[2/5](360/1834) ||  training loss 0.01691 || training accuracy 77.50% || lr [9.524135262331675e-05]\n",
            "Epoch[2/5](380/1834) ||  training loss 0.01858 || training accuracy 79.06% || lr [8.422735529644883e-05]\n",
            "Epoch[2/5](400/1834) ||  training loss 0.01875 || training accuracy 76.88% || lr [2.591231629491857e-05]\n",
            "Epoch[2/5](420/1834) ||  training loss 0.01894 || training accuracy 74.53% || lr [8.856374635655863e-07]\n",
            "Epoch[2/5](440/1834) ||  training loss 0.01986 || training accuracy 75.94% || lr [4.3733338321781665e-05]\n",
            "Epoch[2/5](460/1834) ||  training loss 0.01722 || training accuracy 78.44% || lr [9.524135262329549e-05]\n",
            "Epoch[2/5](480/1834) ||  training loss 0.01676 || training accuracy 78.91% || lr [8.422735529642968e-05]\n",
            "Epoch[2/5](500/1834) ||  training loss 0.01767 || training accuracy 77.81% || lr [2.5912316294913704e-05]\n",
            "Epoch[2/5](520/1834) ||  training loss 0.0188 || training accuracy 75.16% || lr [8.856374635651872e-07]\n",
            "Epoch[2/5](540/1834) ||  training loss 0.01772 || training accuracy 78.44% || lr [4.373333832177216e-05]\n",
            "Epoch[2/5](560/1834) ||  training loss 0.01671 || training accuracy 80.47% || lr [9.52413526232742e-05]\n",
            "Epoch[2/5](580/1834) ||  training loss 0.01853 || training accuracy 76.09% || lr [8.422735529641153e-05]\n",
            "Epoch[2/5](600/1834) ||  training loss 0.0176 || training accuracy 77.81% || lr [2.591231629490757e-05]\n",
            "Epoch[2/5](620/1834) ||  training loss 0.0181 || training accuracy 77.19% || lr [8.856374635650988e-07]\n",
            "Epoch[2/5](640/1834) ||  training loss 0.01677 || training accuracy 78.44% || lr [4.3733338321765124e-05]\n",
            "Epoch[2/5](660/1834) ||  training loss 0.01865 || training accuracy 75.78% || lr [9.524135262325767e-05]\n",
            "Epoch[2/5](680/1834) ||  training loss 0.01663 || training accuracy 79.38% || lr [8.422735529639704e-05]\n",
            "Epoch[2/5](700/1834) ||  training loss 0.01838 || training accuracy 77.03% || lr [2.5912316294902892e-05]\n",
            "Epoch[2/5](720/1834) ||  training loss 0.01903 || training accuracy 75.78% || lr [8.856374635657632e-07]\n",
            "Epoch[2/5](740/1834) ||  training loss 0.01609 || training accuracy 80.00% || lr [4.373333832179356e-05]\n",
            "Epoch[2/5](760/1834) ||  training loss 0.01831 || training accuracy 77.97% || lr [9.524135262332208e-05]\n",
            "Epoch[2/5](780/1834) ||  training loss 0.01816 || training accuracy 75.78% || lr [8.422735529645363e-05]\n",
            "Epoch[2/5](800/1834) ||  training loss 0.01665 || training accuracy 77.97% || lr [2.5912316294920063e-05]\n",
            "Epoch[2/5](820/1834) ||  training loss 0.01733 || training accuracy 78.75% || lr [8.856374635661613e-07]\n",
            "Epoch[2/5](840/1834) ||  training loss 0.01758 || training accuracy 77.03% || lr [4.3733338321823434e-05]\n",
            "Epoch[2/5](860/1834) ||  training loss 0.01643 || training accuracy 78.12% || lr [9.524135262338656e-05]\n",
            "Epoch[2/5](880/1834) ||  training loss 0.02007 || training accuracy 73.91% || lr [8.42273552965103e-05]\n",
            "Epoch[2/5](900/1834) ||  training loss 0.01898 || training accuracy 75.78% || lr [2.5912316294937292e-05]\n",
            "Epoch[2/5](920/1834) ||  training loss 0.01731 || training accuracy 77.19% || lr [8.856374635660231e-07]\n",
            "Epoch[2/5](940/1834) ||  training loss 0.01766 || training accuracy 79.69% || lr [4.373333832181393e-05]\n",
            "Epoch[2/5](960/1834) ||  training loss 0.01909 || training accuracy 74.22% || lr [9.524135262336533e-05]\n",
            "Epoch[2/5](980/1834) ||  training loss 0.01978 || training accuracy 74.06% || lr [8.422735529649218e-05]\n",
            "Epoch[2/5](1000/1834) ||  training loss 0.01947 || training accuracy 74.84% || lr [2.591231629493242e-05]\n",
            "Epoch[2/5](1020/1834) ||  training loss 0.01767 || training accuracy 77.81% || lr [8.856374635658904e-07]\n",
            "Epoch[2/5](1040/1834) ||  training loss 0.01866 || training accuracy 77.66% || lr [4.3733338321804426e-05]\n",
            "Epoch[2/5](1060/1834) ||  training loss 0.019 || training accuracy 73.75% || lr [9.524135262334401e-05]\n",
            "Epoch[2/5](1080/1834) ||  training loss 0.01836 || training accuracy 76.41% || lr [8.422735529647294e-05]\n",
            "Epoch[2/5](1100/1834) ||  training loss 0.01865 || training accuracy 75.94% || lr [2.591231629492627e-05]\n",
            "Epoch[2/5](1120/1834) ||  training loss 0.01734 || training accuracy 79.06% || lr [8.856374635657521e-07]\n",
            "Epoch[2/5](1140/1834) ||  training loss 0.01782 || training accuracy 77.03% || lr [4.3733338321794905e-05]\n",
            "Epoch[2/5](1160/1834) ||  training loss 0.01749 || training accuracy 77.03% || lr [9.52413526233221e-05]\n",
            "Epoch[2/5](1180/1834) ||  training loss 0.01668 || training accuracy 79.22% || lr [8.422735529645372e-05]\n",
            "Epoch[2/5](1200/1834) ||  training loss 0.01824 || training accuracy 76.88% || lr [2.591231629492014e-05]\n",
            "Epoch[2/5](1220/1834) ||  training loss 0.01624 || training accuracy 79.53% || lr [8.856374635653529e-07]\n",
            "Epoch[2/5](1240/1834) ||  training loss 0.01602 || training accuracy 79.69% || lr [4.373333832178541e-05]\n",
            "Epoch[2/5](1260/1834) ||  training loss 0.01804 || training accuracy 76.56% || lr [9.524135262330074e-05]\n",
            "Epoch[2/5](1280/1834) ||  training loss 0.01752 || training accuracy 78.59% || lr [8.422735529643445e-05]\n",
            "Epoch[2/5](1300/1834) ||  training loss 0.01893 || training accuracy 76.88% || lr [2.5912316294913985e-05]\n",
            "Epoch[2/5](1320/1834) ||  training loss 0.01674 || training accuracy 77.66% || lr [8.856374635652702e-07]\n",
            "Epoch[2/5](1340/1834) ||  training loss 0.01708 || training accuracy 80.00% || lr [4.3733338321776975e-05]\n",
            "Epoch[2/5](1360/1834) ||  training loss 0.01919 || training accuracy 77.97% || lr [9.524135262328488e-05]\n",
            "Epoch[2/5](1380/1834) ||  training loss 0.01748 || training accuracy 77.97% || lr [8.422735529642109e-05]\n",
            "Epoch[2/5](1400/1834) ||  training loss 0.0184 || training accuracy 76.88% || lr [2.5912316294909333e-05]\n",
            "Epoch[2/5](1420/1834) ||  training loss 0.01784 || training accuracy 77.19% || lr [8.856374635659292e-07]\n",
            "Epoch[2/5](1440/1834) ||  training loss 0.0162 || training accuracy 80.94% || lr [4.3733338321805415e-05]\n",
            "Epoch[2/5](1460/1834) ||  training loss 0.01835 || training accuracy 77.03% || lr [9.52413526233493e-05]\n",
            "Epoch[2/5](1480/1834) ||  training loss 0.01681 || training accuracy 81.09% || lr [8.422735529647766e-05]\n",
            "Epoch[2/5](1500/1834) ||  training loss 0.01678 || training accuracy 78.44% || lr [2.591231629492651e-05]\n",
            "Epoch[2/5](1520/1834) ||  training loss 0.0169 || training accuracy 77.50% || lr [8.856374635657965e-07]\n",
            "Epoch[2/5](1540/1834) ||  training loss 0.01609 || training accuracy 79.84% || lr [4.373333832179594e-05]\n",
            "Epoch[2/5](1560/1834) ||  training loss 0.01866 || training accuracy 75.31% || lr [9.524135262332807e-05]\n",
            "Epoch[2/5](1580/1834) ||  training loss 0.0187 || training accuracy 75.31% || lr [8.422735529645849e-05]\n",
            "Epoch[2/5](1600/1834) ||  training loss 0.01671 || training accuracy 77.03% || lr [2.59123162949229e-05]\n",
            "Epoch[2/5](1620/1834) ||  training loss 0.01792 || training accuracy 77.97% || lr [8.856374635656582e-07]\n",
            "Epoch[2/5](1640/1834) ||  training loss 0.01594 || training accuracy 77.97% || lr [4.373333832178641e-05]\n",
            "Epoch[2/5](1660/1834) ||  training loss 0.01617 || training accuracy 80.62% || lr [9.52413526233067e-05]\n",
            "Epoch[2/5](1680/1834) ||  training loss 0.02034 || training accuracy 76.09% || lr [8.422735529643918e-05]\n",
            "Epoch[2/5](1700/1834) ||  training loss 0.01864 || training accuracy 74.69% || lr [2.591231629491673e-05]\n",
            "Epoch[2/5](1720/1834) ||  training loss 0.01718 || training accuracy 79.06% || lr [8.856374635655253e-07]\n",
            "Epoch[2/5](1740/1834) ||  training loss 0.01892 || training accuracy 75.94% || lr [4.37333383217769e-05]\n",
            "Epoch[2/5](1760/1834) ||  training loss 0.01825 || training accuracy 77.34% || lr [9.52413526232842e-05]\n",
            "Epoch[2/5](1780/1834) ||  training loss 0.01901 || training accuracy 77.97% || lr [8.422735529641999e-05]\n",
            "Epoch[2/5](1800/1834) ||  training loss 0.01924 || training accuracy 76.25% || lr [2.5912316294910594e-05]\n",
            "Epoch[2/5](1820/1834) ||  training loss 0.01755 || training accuracy 78.12% || lr [8.856374635656516e-07]\n",
            "Epoch[3/5](20/1834) ||  training loss 0.01722 || training accuracy 78.75% || lr [8.422735529647656e-05]\n",
            "Epoch[3/5](40/1834) ||  training loss 0.01645 || training accuracy 79.06% || lr [9.524135262334939e-05]\n",
            "Epoch[3/5](60/1834) ||  training loss 0.0182 || training accuracy 77.34% || lr [4.3733338321806974e-05]\n",
            "Epoch[3/5](80/1834) ||  training loss 0.01677 || training accuracy 78.28% || lr [8.856374635662231e-07]\n",
            "Epoch[3/5](100/1834) ||  training loss 0.01543 || training accuracy 81.09% || lr [2.5912316294922113e-05]\n",
            "Epoch[3/5](120/1834) ||  training loss 0.01515 || training accuracy 80.31% || lr [8.422735529645785e-05]\n",
            "Epoch[3/5](140/1834) ||  training loss 0.01858 || training accuracy 76.25% || lr [9.524135262332848e-05]\n",
            "Epoch[3/5](160/1834) ||  training loss 0.01796 || training accuracy 78.12% || lr [4.3733338321796816e-05]\n",
            "Epoch[3/5](180/1834) ||  training loss 0.016 || training accuracy 80.00% || lr [8.856374635659579e-07]\n",
            "Epoch[3/5](200/1834) ||  training loss 0.01853 || training accuracy 76.72% || lr [2.5912316294916774e-05]\n",
            "Epoch[3/5](220/1834) ||  training loss 0.0159 || training accuracy 79.84% || lr [8.422735529644386e-05]\n",
            "Epoch[3/5](240/1834) ||  training loss 0.01482 || training accuracy 82.97% || lr [9.524135262331215e-05]\n",
            "Epoch[3/5](260/1834) ||  training loss 0.01928 || training accuracy 76.72% || lr [4.3733338321789044e-05]\n",
            "Epoch[3/5](280/1834) ||  training loss 0.01678 || training accuracy 77.81% || lr [8.856374635657466e-07]\n",
            "Epoch[3/5](300/1834) ||  training loss 0.0147 || training accuracy 82.34% || lr [2.591231629493334e-05]\n",
            "Epoch[3/5](320/1834) ||  training loss 0.01534 || training accuracy 81.25% || lr [8.422735529650097e-05]\n",
            "Epoch[3/5](340/1834) ||  training loss 0.01806 || training accuracy 76.72% || lr [9.524135262337633e-05]\n",
            "Epoch[3/5](360/1834) ||  training loss 0.01721 || training accuracy 76.72% || lr [4.373333832181826e-05]\n",
            "Epoch[3/5](380/1834) ||  training loss 0.01661 || training accuracy 82.03% || lr [8.856374635662781e-07]\n",
            "Epoch[3/5](400/1834) ||  training loss 0.01598 || training accuracy 79.84% || lr [2.59123162949278e-05]\n",
            "Epoch[3/5](420/1834) ||  training loss 0.01653 || training accuracy 80.16% || lr [8.422735529648228e-05]\n",
            "Epoch[3/5](440/1834) ||  training loss 0.01653 || training accuracy 78.28% || lr [9.524135262335535e-05]\n",
            "Epoch[3/5](460/1834) ||  training loss 0.01839 || training accuracy 77.66% || lr [4.373333832181091e-05]\n",
            "Epoch[3/5](480/1834) ||  training loss 0.01717 || training accuracy 79.22% || lr [8.856374635662842e-07]\n",
            "Epoch[3/5](500/1834) ||  training loss 0.01555 || training accuracy 79.38% || lr [2.5912316294922262e-05]\n",
            "Epoch[3/5](520/1834) ||  training loss 0.01586 || training accuracy 79.69% || lr [8.422735529646246e-05]\n",
            "Epoch[3/5](540/1834) ||  training loss 0.01691 || training accuracy 78.75% || lr [9.524135262333307e-05]\n",
            "Epoch[3/5](560/1834) ||  training loss 0.01934 || training accuracy 76.09% || lr [4.373333832179924e-05]\n",
            "Epoch[3/5](580/1834) ||  training loss 0.01469 || training accuracy 80.62% || lr [8.856374635657507e-07]\n",
            "Epoch[3/5](600/1834) ||  training loss 0.01812 || training accuracy 77.81% || lr [2.5912316294941307e-05]\n",
            "Epoch[3/5](620/1834) ||  training loss 0.01597 || training accuracy 80.31% || lr [8.422735529651858e-05]\n",
            "Epoch[3/5](640/1834) ||  training loss 0.01664 || training accuracy 79.22% || lr [9.524135262339791e-05]\n",
            "Epoch[3/5](660/1834) ||  training loss 0.01791 || training accuracy 78.75% || lr [4.373333832182989e-05]\n",
            "Epoch[3/5](680/1834) ||  training loss 0.01645 || training accuracy 79.53% || lr [8.856374635665546e-07]\n",
            "Epoch[3/5](700/1834) ||  training loss 0.01748 || training accuracy 77.81% || lr [2.5912316294911197e-05]\n",
            "Epoch[3/5](720/1834) ||  training loss 0.0157 || training accuracy 80.00% || lr [8.422735529642506e-05]\n",
            "Epoch[3/5](740/1834) ||  training loss 0.01683 || training accuracy 78.59% || lr [9.524135262328995e-05]\n",
            "Epoch[3/5](760/1834) ||  training loss 0.01763 || training accuracy 77.19% || lr [4.3733338321778913e-05]\n",
            "Epoch[3/5](780/1834) ||  training loss 0.01602 || training accuracy 81.09% || lr [8.85637463565758e-07]\n",
            "Epoch[3/5](800/1834) ||  training loss 0.0165 || training accuracy 78.28% || lr [2.5912316294927748e-05]\n",
            "Epoch[3/5](820/1834) ||  training loss 0.0164 || training accuracy 81.09% || lr [8.422735529648115e-05]\n",
            "Epoch[3/5](840/1834) ||  training loss 0.01648 || training accuracy 79.53% || lr [9.524135262335472e-05]\n",
            "Epoch[3/5](860/1834) ||  training loss 0.01644 || training accuracy 79.69% || lr [4.373333832180951e-05]\n",
            "Epoch[3/5](880/1834) ||  training loss 0.01606 || training accuracy 81.25% || lr [8.85637463566023e-07]\n",
            "Epoch[3/5](900/1834) ||  training loss 0.01536 || training accuracy 81.41% || lr [2.5912316294945773e-05]\n",
            "Epoch[3/5](920/1834) ||  training loss 0.01672 || training accuracy 78.75% || lr [8.42273552965441e-05]\n",
            "Epoch[3/5](940/1834) ||  training loss 0.01599 || training accuracy 79.53% || lr [9.524135262342485e-05]\n",
            "Epoch[3/5](960/1834) ||  training loss 0.01693 || training accuracy 76.25% || lr [4.373333832183975e-05]\n",
            "Epoch[3/5](980/1834) ||  training loss 0.01697 || training accuracy 80.16% || lr [8.856374635668758e-07]\n",
            "Epoch[3/5](1000/1834) ||  training loss 0.01772 || training accuracy 77.34% || lr [2.5912316294918156e-05]\n",
            "Epoch[3/5](1020/1834) ||  training loss 0.0165 || training accuracy 79.69% || lr [8.42273552964485e-05]\n",
            "Epoch[3/5](1040/1834) ||  training loss 0.01736 || training accuracy 77.03% || lr [9.524135262331696e-05]\n",
            "Epoch[3/5](1060/1834) ||  training loss 0.01752 || training accuracy 77.03% || lr [4.373333832179444e-05]\n",
            "Epoch[3/5](1080/1834) ||  training loss 0.01899 || training accuracy 77.50% || lr [8.856374635655466e-07]\n",
            "Epoch[3/5](1100/1834) ||  training loss 0.0161 || training accuracy 79.22% || lr [2.5912316294934714e-05]\n",
            "Epoch[3/5](1120/1834) ||  training loss 0.01552 || training accuracy 83.28% || lr [8.422735529650462e-05]\n",
            "Epoch[3/5](1140/1834) ||  training loss 0.01526 || training accuracy 80.78% || lr [9.524135262338173e-05]\n",
            "Epoch[3/5](1160/1834) ||  training loss 0.01631 || training accuracy 81.09% || lr [4.373333832182222e-05]\n",
            "Epoch[3/5](1180/1834) ||  training loss 0.01567 || training accuracy 81.88% || lr [8.856374635663505e-07]\n",
            "Epoch[3/5](1200/1834) ||  training loss 0.01765 || training accuracy 76.72% || lr [2.5912316294907094e-05]\n",
            "Epoch[3/5](1220/1834) ||  training loss 0.0183 || training accuracy 80.00% || lr [8.42273552964091e-05]\n",
            "Epoch[3/5](1240/1834) ||  training loss 0.02036 || training accuracy 74.06% || lr [9.524135262327385e-05]\n",
            "Epoch[3/5](1260/1834) ||  training loss 0.0184 || training accuracy 78.44% || lr [4.3733338321774116e-05]\n",
            "Epoch[3/5](1280/1834) ||  training loss 0.0158 || training accuracy 78.12% || lr [8.856374635655541e-07]\n",
            "Epoch[3/5](1300/1834) ||  training loss 0.01744 || training accuracy 77.19% || lr [2.5912316294923638e-05]\n",
            "Epoch[3/5](1320/1834) ||  training loss 0.01708 || training accuracy 78.91% || lr [8.422735529646717e-05]\n",
            "Epoch[3/5](1340/1834) ||  training loss 0.0154 || training accuracy 79.38% || lr [9.524135262333859e-05]\n",
            "Epoch[3/5](1360/1834) ||  training loss 0.01546 || training accuracy 80.62% || lr [4.3733338321801885e-05]\n",
            "Epoch[3/5](1380/1834) ||  training loss 0.01614 || training accuracy 79.38% || lr [8.856374635663516e-07]\n",
            "Epoch[3/5](1400/1834) ||  training loss 0.01598 || training accuracy 80.47% || lr [2.5912316294896025e-05]\n",
            "Epoch[3/5](1420/1834) ||  training loss 0.01737 || training accuracy 78.44% || lr [8.422735529637155e-05]\n",
            "Epoch[3/5](1440/1834) ||  training loss 0.01645 || training accuracy 77.81% || lr [9.524135262323177e-05]\n",
            "Epoch[3/5](1460/1834) ||  training loss 0.01793 || training accuracy 76.56% || lr [4.37333383217537e-05]\n",
            "Epoch[3/5](1480/1834) ||  training loss 0.0181 || training accuracy 78.28% || lr [8.85637463565027e-07]\n",
            "Epoch[3/5](1500/1834) ||  training loss 0.0177 || training accuracy 77.34% || lr [2.5912316294912586e-05]\n",
            "Epoch[3/5](1520/1834) ||  training loss 0.01365 || training accuracy 82.19% || lr [8.422735529642981e-05]\n",
            "Epoch[3/5](1540/1834) ||  training loss 0.01588 || training accuracy 78.28% || lr [9.524135262329667e-05]\n",
            "Epoch[3/5](1560/1834) ||  training loss 0.01816 || training accuracy 74.06% || lr [4.373333832178153e-05]\n",
            "Epoch[3/5](1580/1834) ||  training loss 0.01638 || training accuracy 77.19% || lr [8.856374635658256e-07]\n",
            "Epoch[3/5](1600/1834) ||  training loss 0.01631 || training accuracy 78.44% || lr [2.5912316294882486e-05]\n",
            "Epoch[3/5](1620/1834) ||  training loss 0.01649 || training accuracy 79.53% || lr [8.42273552963342e-05]\n",
            "Epoch[3/5](1640/1834) ||  training loss 0.01702 || training accuracy 76.88% || lr [9.524135262318744e-05]\n",
            "Epoch[3/5](1660/1834) ||  training loss 0.01498 || training accuracy 81.41% || lr [4.373333832173332e-05]\n",
            "Epoch[3/5](1680/1834) ||  training loss 0.01676 || training accuracy 78.44% || lr [8.856374635645002e-07]\n",
            "Epoch[3/5](1700/1834) ||  training loss 0.01629 || training accuracy 78.28% || lr [2.5912316294902974e-05]\n",
            "Epoch[3/5](1720/1834) ||  training loss 0.01643 || training accuracy 79.69% || lr [8.422735529639501e-05]\n",
            "Epoch[3/5](1740/1834) ||  training loss 0.01541 || training accuracy 81.09% || lr [9.524135262325874e-05]\n",
            "Epoch[3/5](1760/1834) ||  training loss 0.01679 || training accuracy 80.00% || lr [4.3733338321763565e-05]\n",
            "Epoch[3/5](1780/1834) ||  training loss 0.01702 || training accuracy 79.84% || lr [8.856374635653478e-07]\n",
            "Epoch[3/5](1800/1834) ||  training loss 0.01683 || training accuracy 78.59% || lr [2.5912316294966186e-05]\n",
            "Epoch[3/5](1820/1834) ||  training loss 0.01486 || training accuracy 82.50% || lr [8.422735529660487e-05]\n",
            "Epoch[4/5](20/1834) ||  training loss 0.01568 || training accuracy 80.16% || lr [6.243449435836972e-05]\n",
            "Epoch[4/5](40/1834) ||  training loss 0.01506 || training accuracy 81.88% || lr [7.78360372491662e-06]\n",
            "Epoch[4/5](60/1834) ||  training loss 0.01561 || training accuracy 79.22% || lr [1.1474337861219178e-05]\n",
            "Epoch[4/5](80/1834) ||  training loss 0.01511 || training accuracy 79.69% || lr [6.840622763429601e-05]\n",
            "Epoch[4/5](100/1834) ||  training loss 0.01664 || training accuracy 79.22% || lr [9.990133642150347e-05]\n",
            "Epoch[4/5](120/1834) ||  training loss 0.01576 || training accuracy 80.16% || lr [6.243449435830048e-05]\n",
            "Epoch[4/5](140/1834) ||  training loss 0.01719 || training accuracy 77.50% || lr [7.783603724906918e-06]\n",
            "Epoch[4/5](160/1834) ||  training loss 0.01474 || training accuracy 81.72% || lr [1.1474337861228046e-05]\n",
            "Epoch[4/5](180/1834) ||  training loss 0.01394 || training accuracy 82.19% || lr [6.84062276343438e-05]\n",
            "Epoch[4/5](200/1834) ||  training loss 0.01679 || training accuracy 79.38% || lr [9.990133642157096e-05]\n",
            "Epoch[4/5](220/1834) ||  training loss 0.01657 || training accuracy 79.69% || lr [6.243449435834092e-05]\n",
            "Epoch[4/5](240/1834) ||  training loss 0.01637 || training accuracy 79.22% || lr [7.783603724912749e-06]\n",
            "Epoch[4/5](260/1834) ||  training loss 0.01549 || training accuracy 78.59% || lr [1.1474337861214448e-05]\n",
            "Epoch[4/5](280/1834) ||  training loss 0.01332 || training accuracy 83.12% || lr [6.840622763426316e-05]\n",
            "Epoch[4/5](300/1834) ||  training loss 0.01511 || training accuracy 79.06% || lr [9.990133642145844e-05]\n",
            "Epoch[4/5](320/1834) ||  training loss 0.01493 || training accuracy 80.00% || lr [6.243449435827172e-05]\n",
            "Epoch[4/5](340/1834) ||  training loss 0.0142 || training accuracy 80.78% || lr [7.783603724903047e-06]\n",
            "Epoch[4/5](360/1834) ||  training loss 0.01604 || training accuracy 80.16% || lr [1.1474337861223327e-05]\n",
            "Epoch[4/5](380/1834) ||  training loss 0.01522 || training accuracy 80.16% || lr [6.84062276343084e-05]\n",
            "Epoch[4/5](400/1834) ||  training loss 0.01654 || training accuracy 81.56% || lr [9.990133642152609e-05]\n",
            "Epoch[4/5](420/1834) ||  training loss 0.01546 || training accuracy 80.47% || lr [6.243449435831224e-05]\n",
            "Epoch[4/5](440/1834) ||  training loss 0.01472 || training accuracy 79.84% || lr [7.78360372490889e-06]\n",
            "Epoch[4/5](460/1834) ||  training loss 0.01706 || training accuracy 76.72% || lr [1.1474337861209727e-05]\n",
            "Epoch[4/5](480/1834) ||  training loss 0.01548 || training accuracy 80.16% || lr [6.840622763423306e-05]\n",
            "Epoch[4/5](500/1834) ||  training loss 0.01545 || training accuracy 82.19% || lr [9.990133642141383e-05]\n",
            "Epoch[4/5](520/1834) ||  training loss 0.0162 || training accuracy 77.19% || lr [6.243449435824296e-05]\n",
            "Epoch[4/5](540/1834) ||  training loss 0.01548 || training accuracy 79.84% || lr [7.783603724899173e-06]\n",
            "Epoch[4/5](560/1834) ||  training loss 0.01576 || training accuracy 81.09% || lr [1.1474337861219242e-05]\n",
            "Epoch[4/5](580/1834) ||  training loss 0.01455 || training accuracy 81.56% || lr [6.840622763428205e-05]\n",
            "Epoch[4/5](600/1834) ||  training loss 0.01635 || training accuracy 78.59% || lr [9.99013364214867e-05]\n",
            "Epoch[4/5](620/1834) ||  training loss 0.01437 || training accuracy 80.78% || lr [6.24344943582869e-05]\n",
            "Epoch[4/5](640/1834) ||  training loss 0.01718 || training accuracy 76.72% || lr [7.783603724905442e-06]\n",
            "Epoch[4/5](660/1834) ||  training loss 0.01508 || training accuracy 82.03% || lr [1.1474337861205644e-05]\n",
            "Epoch[4/5](680/1834) ||  training loss 0.01499 || training accuracy 80.00% || lr [6.840622763420681e-05]\n",
            "Epoch[4/5](700/1834) ||  training loss 0.01571 || training accuracy 80.94% || lr [9.990133642137449e-05]\n",
            "Epoch[4/5](720/1834) ||  training loss 0.01691 || training accuracy 80.31% || lr [6.243449435821777e-05]\n",
            "Epoch[4/5](740/1834) ||  training loss 0.01449 || training accuracy 82.50% || lr [7.783603724895756e-06]\n",
            "Epoch[4/5](760/1834) ||  training loss 0.01708 || training accuracy 78.75% || lr [1.1474337861212696e-05]\n",
            "Epoch[4/5](780/1834) ||  training loss 0.01586 || training accuracy 78.59% || lr [6.84062276342519e-05]\n",
            "Epoch[4/5](800/1834) ||  training loss 0.01484 || training accuracy 80.47% || lr [9.990133642144172e-05]\n",
            "Epoch[4/5](820/1834) ||  training loss 0.0156 || training accuracy 78.91% || lr [6.243449435826365e-05]\n",
            "Epoch[4/5](840/1834) ||  training loss 0.01675 || training accuracy 79.69% || lr [7.783603724901575e-06]\n",
            "Epoch[4/5](860/1834) ||  training loss 0.01603 || training accuracy 79.69% || lr [1.1474337861200917e-05]\n",
            "Epoch[4/5](880/1834) ||  training loss 0.01467 || training accuracy 80.62% || lr [6.840622763417663e-05]\n",
            "Epoch[4/5](900/1834) ||  training loss 0.01778 || training accuracy 77.81% || lr [9.990133642132947e-05]\n",
            "Epoch[4/5](920/1834) ||  training loss 0.01629 || training accuracy 78.75% || lr [6.243449435819174e-05]\n",
            "Epoch[4/5](940/1834) ||  training loss 0.0155 || training accuracy 78.91% || lr [7.783603724891882e-06]\n",
            "Epoch[4/5](960/1834) ||  training loss 0.017 || training accuracy 78.28% || lr [1.1474337861207973e-05]\n",
            "Epoch[4/5](980/1834) ||  training loss 0.01434 || training accuracy 83.28% || lr [6.840622763422178e-05]\n",
            "Epoch[4/5](1000/1834) ||  training loss 0.01496 || training accuracy 82.34% || lr [9.990133642139675e-05]\n",
            "Epoch[4/5](1020/1834) ||  training loss 0.01723 || training accuracy 78.12% || lr [6.243449435823488e-05]\n",
            "Epoch[4/5](1040/1834) ||  training loss 0.01755 || training accuracy 76.41% || lr [7.783603724899225e-06]\n",
            "Epoch[4/5](1060/1834) ||  training loss 0.01725 || training accuracy 80.62% || lr [1.1474337861196181e-05]\n",
            "Epoch[4/5](1080/1834) ||  training loss 0.01373 || training accuracy 82.66% || lr [6.84062276341464e-05]\n",
            "Epoch[4/5](1100/1834) ||  training loss 0.01628 || training accuracy 79.06% || lr [9.990133642128435e-05]\n",
            "Epoch[4/5](1120/1834) ||  training loss 0.01611 || training accuracy 79.53% || lr [6.24344943581629e-05]\n",
            "Epoch[4/5](1140/1834) ||  training loss 0.01581 || training accuracy 80.62% || lr [7.783603724891041e-06]\n",
            "Epoch[4/5](1160/1834) ||  training loss 0.01627 || training accuracy 77.66% || lr [1.1474337861223903e-05]\n",
            "Epoch[4/5](1180/1834) ||  training loss 0.01435 || training accuracy 80.00% || lr [6.84062276343147e-05]\n",
            "Epoch[4/5](1200/1834) ||  training loss 0.01456 || training accuracy 82.03% || lr [9.99013364215315e-05]\n",
            "Epoch[4/5](1220/1834) ||  training loss 0.01352 || training accuracy 82.19% || lr [6.243449435831845e-05]\n",
            "Epoch[4/5](1240/1834) ||  training loss 0.01704 || training accuracy 79.22% || lr [7.783603724909357e-06]\n",
            "Epoch[4/5](1260/1834) ||  training loss 0.01477 || training accuracy 79.69% || lr [1.1474337861231612e-05]\n",
            "Epoch[4/5](1280/1834) ||  training loss 0.01485 || training accuracy 79.69% || lr [6.840622763436643e-05]\n",
            "Epoch[4/5](1300/1834) ||  training loss 0.01574 || training accuracy 80.16% || lr [9.990133642160468e-05]\n",
            "Epoch[4/5](1320/1834) ||  training loss 0.01551 || training accuracy 79.22% || lr [6.243449435836245e-05]\n",
            "Epoch[4/5](1340/1834) ||  training loss 0.01559 || training accuracy 80.78% || lr [7.783603724915634e-06]\n",
            "Epoch[4/5](1360/1834) ||  training loss 0.01528 || training accuracy 81.72% || lr [1.1474337861218014e-05]\n",
            "Epoch[4/5](1380/1834) ||  training loss 0.01658 || training accuracy 79.69% || lr [6.840622763428848e-05]\n",
            "Epoch[4/5](1400/1834) ||  training loss 0.0171 || training accuracy 79.53% || lr [9.990133642149211e-05]\n",
            "Epoch[4/5](1420/1834) ||  training loss 0.01649 || training accuracy 78.12% || lr [6.243449435829323e-05]\n",
            "Epoch[4/5](1440/1834) ||  training loss 0.01691 || training accuracy 78.75% || lr [7.783603724905932e-06]\n",
            "Epoch[4/5](1460/1834) ||  training loss 0.0141 || training accuracy 81.41% || lr [1.1474337861226886e-05]\n",
            "Epoch[4/5](1480/1834) ||  training loss 0.01542 || training accuracy 79.84% || lr [6.840622763433106e-05]\n",
            "Epoch[4/5](1500/1834) ||  training loss 0.01385 || training accuracy 81.88% || lr [9.990133642155982e-05]\n",
            "Epoch[4/5](1520/1834) ||  training loss 0.01572 || training accuracy 80.47% || lr [6.243449435833378e-05]\n",
            "Epoch[4/5](1540/1834) ||  training loss 0.01633 || training accuracy 80.00% || lr [7.78360372491178e-06]\n",
            "Epoch[4/5](1560/1834) ||  training loss 0.0151 || training accuracy 81.88% || lr [1.1474337861213282e-05]\n",
            "Epoch[4/5](1580/1834) ||  training loss 0.01495 || training accuracy 81.25% || lr [6.840622763425567e-05]\n",
            "Epoch[4/5](1600/1834) ||  training loss 0.01475 || training accuracy 80.16% || lr [9.99013364214472e-05]\n",
            "Epoch[4/5](1620/1834) ||  training loss 0.0169 || training accuracy 77.03% || lr [6.243449435826451e-05]\n",
            "Epoch[4/5](1640/1834) ||  training loss 0.01363 || training accuracy 82.97% || lr [7.783603724902065e-06]\n",
            "Epoch[4/5](1660/1834) ||  training loss 0.01549 || training accuracy 80.00% || lr [1.147433786122215e-05]\n",
            "Epoch[4/5](1680/1834) ||  training loss 0.01603 || training accuracy 79.22% || lr [6.840622763430081e-05]\n",
            "Epoch[4/5](1700/1834) ||  training loss 0.01581 || training accuracy 81.56% || lr [9.990133642151459e-05]\n",
            "Epoch[4/5](1720/1834) ||  training loss 0.01697 || training accuracy 79.22% || lr [6.243449435830487e-05]\n",
            "Epoch[4/5](1740/1834) ||  training loss 0.01452 || training accuracy 82.97% || lr [7.783603724907885e-06]\n",
            "Epoch[4/5](1760/1834) ||  training loss 0.0171 || training accuracy 77.81% || lr [1.1474337861208558e-05]\n",
            "Epoch[4/5](1780/1834) ||  training loss 0.01517 || training accuracy 80.62% || lr [6.840622763422551e-05]\n",
            "Epoch[4/5](1800/1834) ||  training loss 0.01552 || training accuracy 81.72% || lr [9.990133642140245e-05]\n",
            "Epoch[4/5](1820/1834) ||  training loss 0.01577 || training accuracy 80.16% || lr [6.24344943582357e-05]\n",
            "Epoch[5/5](20/1834) ||  training loss 0.01396 || training accuracy 82.50% || lr [2.4471741852436112e-06]\n",
            "Epoch[5/5](40/1834) ||  training loss 0.01444 || training accuracy 81.72% || lr [5.000000000002822e-05]\n",
            "Epoch[5/5](60/1834) ||  training loss 0.0139 || training accuracy 81.72% || lr [9.755282581481272e-05]\n",
            "Epoch[5/5](80/1834) ||  training loss 0.01461 || training accuracy 81.88% || lr [7.938926261466997e-05]\n",
            "Epoch[5/5](100/1834) ||  training loss 0.01388 || training accuracy 83.44% || lr [2.0610737385389312e-05]\n",
            "Epoch[5/5](120/1834) ||  training loss 0.01397 || training accuracy 82.34% || lr [2.447174185240525e-06]\n",
            "Epoch[5/5](140/1834) ||  training loss 0.01337 || training accuracy 82.19% || lr [4.999999999997371e-05]\n",
            "Epoch[5/5](160/1834) ||  training loss 0.01392 || training accuracy 82.66% || lr [9.755282581470255e-05]\n",
            "Epoch[5/5](180/1834) ||  training loss 0.01424 || training accuracy 83.75% || lr [7.93892626145792e-05]\n",
            "Epoch[5/5](200/1834) ||  training loss 0.014 || training accuracy 80.31% || lr [2.0610737385366987e-05]\n",
            "Epoch[5/5](220/1834) ||  training loss 0.01424 || training accuracy 80.94% || lr [2.4471741852419815e-06]\n",
            "Epoch[5/5](240/1834) ||  training loss 0.015 || training accuracy 82.19% || lr [5.0000000000009184e-05]\n",
            "Epoch[5/5](260/1834) ||  training loss 0.01338 || training accuracy 80.94% || lr [9.755282581477446e-05]\n",
            "Epoch[5/5](280/1834) ||  training loss 0.01373 || training accuracy 83.28% || lr [7.938926261463816e-05]\n",
            "Epoch[5/5](300/1834) ||  training loss 0.01496 || training accuracy 81.09% || lr [2.0610737385380638e-05]\n",
            "Epoch[5/5](320/1834) ||  training loss 0.01529 || training accuracy 80.31% || lr [2.4471741852397716e-06]\n",
            "Epoch[5/5](340/1834) ||  training loss 0.01308 || training accuracy 83.91% || lr [4.999999999994903e-05]\n",
            "Epoch[5/5](360/1834) ||  training loss 0.0128 || training accuracy 82.81% || lr [9.755282581466432e-05]\n",
            "Epoch[5/5](380/1834) ||  training loss 0.01398 || training accuracy 82.81% || lr [7.938926261454737e-05]\n",
            "Epoch[5/5](400/1834) ||  training loss 0.01254 || training accuracy 84.69% || lr [2.0610737385358324e-05]\n",
            "Epoch[5/5](420/1834) ||  training loss 0.01502 || training accuracy 81.56% || lr [2.447174185241091e-06]\n",
            "Epoch[5/5](440/1834) ||  training loss 0.01414 || training accuracy 81.41% || lr [4.999999999998454e-05]\n",
            "Epoch[5/5](460/1834) ||  training loss 0.01349 || training accuracy 83.59% || lr [9.75528258147308e-05]\n",
            "Epoch[5/5](480/1834) ||  training loss 0.01291 || training accuracy 83.28% || lr [7.938926261460191e-05]\n",
            "Epoch[5/5](500/1834) ||  training loss 0.01427 || training accuracy 82.50% || lr [2.0610737385370813e-05]\n",
            "Epoch[5/5](520/1834) ||  training loss 0.0142 || training accuracy 80.62% || lr [2.447174185247693e-06]\n",
            "Epoch[5/5](540/1834) ||  training loss 0.01297 || training accuracy 82.03% || lr [5.0000000000107244e-05]\n",
            "Epoch[5/5](560/1834) ||  training loss 0.01329 || training accuracy 82.34% || lr [9.75528258149711e-05]\n",
            "Epoch[5/5](580/1834) ||  training loss 0.01311 || training accuracy 82.50% || lr [7.938926261479701e-05]\n",
            "Epoch[5/5](600/1834) ||  training loss 0.01569 || training accuracy 80.31% || lr [2.061073738542272e-05]\n",
            "Epoch[5/5](620/1834) ||  training loss 0.01458 || training accuracy 81.88% || lr [2.4471741852446065e-06]\n",
            "Epoch[5/5](640/1834) ||  training loss 0.0161 || training accuracy 78.12% || lr [5.0000000000052735e-05]\n",
            "Epoch[5/5](660/1834) ||  training loss 0.0129 || training accuracy 83.59% || lr [9.755282581486096e-05]\n",
            "Epoch[5/5](680/1834) ||  training loss 0.01457 || training accuracy 81.88% || lr [7.93892626147085e-05]\n",
            "Epoch[5/5](700/1834) ||  training loss 0.01335 || training accuracy 83.12% || lr [2.0610737385398084e-05]\n",
            "Epoch[5/5](720/1834) ||  training loss 0.01267 || training accuracy 85.00% || lr [2.4471741852459257e-06]\n",
            "Epoch[5/5](740/1834) ||  training loss 0.01422 || training accuracy 81.41% || lr [5.000000000008543e-05]\n",
            "Epoch[5/5](760/1834) ||  training loss 0.01485 || training accuracy 80.16% || lr [9.755282581492743e-05]\n",
            "Epoch[5/5](780/1834) ||  training loss 0.01392 || training accuracy 82.50% || lr [7.938926261476534e-05]\n",
            "Epoch[5/5](800/1834) ||  training loss 0.01481 || training accuracy 82.50% || lr [2.061073738541289e-05]\n",
            "Epoch[5/5](820/1834) ||  training loss 0.0142 || training accuracy 81.25% || lr [2.4471741852473826e-06]\n",
            "Epoch[5/5](840/1834) ||  training loss 0.01478 || training accuracy 82.34% || lr [5.000000000012378e-05]\n",
            "Epoch[5/5](860/1834) ||  training loss 0.01208 || training accuracy 83.28% || lr [9.755282581499842e-05]\n",
            "Epoch[5/5](880/1834) ||  training loss 0.0153 || training accuracy 81.56% || lr [7.938926261482426e-05]\n",
            "Epoch[5/5](900/1834) ||  training loss 0.01496 || training accuracy 80.47% || lr [2.0610737385428838e-05]\n",
            "Epoch[5/5](920/1834) ||  training loss 0.0144 || training accuracy 81.88% || lr [2.4471741852416495e-06]\n",
            "Epoch[5/5](940/1834) ||  training loss 0.01238 || training accuracy 84.38% || lr [4.999999999997639e-05]\n",
            "Epoch[5/5](960/1834) ||  training loss 0.01451 || training accuracy 80.31% || lr [9.755282581471272e-05]\n",
            "Epoch[5/5](980/1834) ||  training loss 0.01297 || training accuracy 82.34% || lr [7.93892626145883e-05]\n",
            "Epoch[5/5](1000/1834) ||  training loss 0.01453 || training accuracy 82.03% || lr [2.0610737385364816e-05]\n",
            "Epoch[5/5](1020/1834) ||  training loss 0.01239 || training accuracy 84.06% || lr [2.4471741852429688e-06]\n",
            "Epoch[5/5](1040/1834) ||  training loss 0.01654 || training accuracy 80.16% || lr [5.000000000000906e-05]\n",
            "Epoch[5/5](1060/1834) ||  training loss 0.01399 || training accuracy 81.25% || lr [9.75528258147799e-05]\n",
            "Epoch[5/5](1080/1834) ||  training loss 0.01444 || training accuracy 82.19% || lr [7.938926261464263e-05]\n",
            "Epoch[5/5](1100/1834) ||  training loss 0.01492 || training accuracy 78.75% || lr [2.061073738538418e-05]\n",
            "Epoch[5/5](1120/1834) ||  training loss 0.01565 || training accuracy 80.31% || lr [2.447174185244288e-06]\n",
            "Epoch[5/5](1140/1834) ||  training loss 0.01604 || training accuracy 81.41% || lr [5.000000000004744e-05]\n",
            "Epoch[5/5](1160/1834) ||  training loss 0.01315 || training accuracy 82.50% || lr [9.755282581484549e-05]\n",
            "Epoch[5/5](1180/1834) ||  training loss 0.01605 || training accuracy 81.09% || lr [7.938926261469718e-05]\n",
            "Epoch[5/5](1200/1834) ||  training loss 0.01531 || training accuracy 79.53% || lr [2.0610737385398978e-05]\n",
            "Epoch[5/5](1220/1834) ||  training loss 0.01476 || training accuracy 81.09% || lr [2.4471741852456073e-06]\n",
            "Epoch[5/5](1240/1834) ||  training loss 0.01396 || training accuracy 81.88% || lr [5.000000000008011e-05]\n",
            "Epoch[5/5](1260/1834) ||  training loss 0.01406 || training accuracy 82.97% || lr [9.755282581491106e-05]\n",
            "Epoch[5/5](1280/1834) ||  training loss 0.01438 || training accuracy 81.88% || lr [7.938926261475168e-05]\n",
            "Epoch[5/5](1300/1834) ||  training loss 0.01269 || training accuracy 84.53% || lr [2.061073738540918e-05]\n",
            "Epoch[5/5](1320/1834) ||  training loss 0.01456 || training accuracy 82.19% || lr [2.447174185238114e-06]\n",
            "Epoch[5/5](1340/1834) ||  training loss 0.01492 || training accuracy 80.78% || lr [4.9999999999932734e-05]\n",
            "Epoch[5/5](1360/1834) ||  training loss 0.01371 || training accuracy 81.25% || lr [9.755282581462522e-05]\n",
            "Epoch[5/5](1380/1834) ||  training loss 0.01433 || training accuracy 82.97% || lr [7.938926261451563e-05]\n",
            "Epoch[5/5](1400/1834) ||  training loss 0.01582 || training accuracy 80.16% || lr [2.0610737385349725e-05]\n",
            "Epoch[5/5](1420/1834) ||  training loss 0.0152 || training accuracy 82.03% || lr [2.447174185239433e-06]\n",
            "Epoch[5/5](1440/1834) ||  training loss 0.0143 || training accuracy 82.50% || lr [4.999999999996541e-05]\n",
            "Epoch[5/5](1460/1834) ||  training loss 0.01399 || training accuracy 81.56% || lr [9.755282581469251e-05]\n",
            "Epoch[5/5](1480/1834) ||  training loss 0.01415 || training accuracy 81.56% || lr [7.938926261457013e-05]\n",
            "Epoch[5/5](1500/1834) ||  training loss 0.01547 || training accuracy 78.59% || lr [2.0610737385364524e-05]\n",
            "Epoch[5/5](1520/1834) ||  training loss 0.01587 || training accuracy 80.47% || lr [2.4471741852425063e-06]\n",
            "Epoch[5/5](1540/1834) ||  training loss 0.01429 || training accuracy 81.09% || lr [4.9999999999998105e-05]\n",
            "Epoch[5/5](1560/1834) ||  training loss 0.01371 || training accuracy 84.22% || lr [9.755282581475807e-05]\n",
            "Epoch[5/5](1580/1834) ||  training loss 0.01307 || training accuracy 83.91% || lr [7.938926261462465e-05]\n",
            "Epoch[5/5](1600/1834) ||  training loss 0.01431 || training accuracy 82.19% || lr [2.0610737385379327e-05]\n",
            "Epoch[5/5](1620/1834) ||  training loss 0.01298 || training accuracy 83.91% || lr [2.4471741852439636e-06]\n",
            "Epoch[5/5](1640/1834) ||  training loss 0.01402 || training accuracy 82.50% || lr [5.000000000003362e-05]\n",
            "Epoch[5/5](1660/1834) ||  training loss 0.01307 || training accuracy 83.75% || lr [9.755282581482908e-05]\n",
            "Epoch[5/5](1680/1834) ||  training loss 0.01307 || training accuracy 83.59% || lr [7.938926261468352e-05]\n",
            "Epoch[5/5](1700/1834) ||  training loss 0.01419 || training accuracy 82.34% || lr [2.061073738539066e-05]\n",
            "Epoch[5/5](1720/1834) ||  training loss 0.01345 || training accuracy 83.28% || lr [2.447174185245283e-06]\n",
            "Epoch[5/5](1740/1834) ||  training loss 0.01399 || training accuracy 81.72% || lr [5.0000000000066274e-05]\n",
            "Epoch[5/5](1760/1834) ||  training loss 0.01474 || training accuracy 81.25% || lr [9.75528258148946e-05]\n",
            "Epoch[5/5](1780/1834) ||  training loss 0.01556 || training accuracy 80.16% || lr [7.938926261473338e-05]\n",
            "Epoch[5/5](1800/1834) ||  training loss 0.01494 || training accuracy 80.31% || lr [2.061073738540543e-05]\n",
            "Epoch[5/5](1820/1834) ||  training loss 0.01448 || training accuracy 82.34% || lr [2.447174185246602e-06]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qBKTwUb722d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}