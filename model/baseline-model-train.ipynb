{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline-model-train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junhyeokk/boomhill24/blob/main/model/baseline-model-train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nG8cgSvr3Iw",
        "outputId": "25cfadad-ecb1-4bf2-cb12-bdf269c3ff77"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PssSplM2wemG"
      },
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image as PIL_Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, resnet34"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8zWtPW8JY5O"
      },
      "source": [
        "seed = 365\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)    \n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbCoXUpEs3tm"
      },
      "source": [
        "drivepath = \"/content/gdrive/MyDrive/simple_data\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfQlU0pjQu2t",
        "outputId": "930b5a74-e346-434d-d659-3dd9cab91850"
      },
      "source": [
        "device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device_type = \"cpu\"\n",
        "device = torch.device(device_type)\n",
        "print(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2_nYsUu0iAc"
      },
      "source": [
        "class KartModel1(nn.Module):\n",
        "  def __init__(self, class_num = 8):\n",
        "    super(KartModel1, self).__init__()\n",
        "    self.class_num = class_num\n",
        "    self.backbone = resnet34(pretrained=True)\n",
        "    self.backbone.fc = nn.Sequential(\n",
        "      nn.Linear(in_features=512, out_features=256, bias=True),\n",
        "      nn.BatchNorm1d(256),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(in_features=256, out_features=class_num, bias=True),\n",
        "      # nn.Softmax(dim=1)\n",
        "      # nn.Sigmoid(),\n",
        "    )\n",
        "  \n",
        "  def forward(self, input_image):\n",
        "    output = self.backbone(input_image)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA4jcJPz7xfL"
      },
      "source": [
        "class KartDataSet1(data.Dataset):\n",
        "  def __init__(self, csv_files):\n",
        "    self.images = []\n",
        "    self.labels = []\n",
        "\n",
        "    for csv_file in csv_files:\n",
        "      with open(csv_file) as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        # next(csv_reader, None)        # 첫번째 row 스킵\n",
        "        \n",
        "        for row in csv_reader:\n",
        "          self.images.append(drivepath + '/csvs/' + row[0])\n",
        "          # self.labels.append([int(x) for x in list(row[1][:3])])\n",
        "          self.labels.append(int(row[1][:3], 2))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.images[index]\n",
        "    image = PIL_Image.open(image_path)\n",
        "\n",
        "    label = self.labels[index]\n",
        "\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((224, 224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),     # -1 ~ 1 로 normalize\n",
        "    ])\n",
        "    \n",
        "    return preprocess(image), label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsv0biYQq_0l",
        "outputId": "56e47dd8-b5e1-4a13-e27a-c849abeebaf8"
      },
      "source": [
        "train_dataset = KartDataSet1([f\"{drivepath}/csvs/data_test_{i}.csv\" for i in [1, 2, 3, 4, 5, 6, 8, 9]])\n",
        "validation_dataset = KartDataSet1([f\"{drivepath}/csvs/data_test_0.csv\"])\n",
        "print(train_dataset[0][0])\n",
        "print(train_dataset[0][1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.0196, -0.0118,  0.0039,  ..., -0.4353, -0.4588, -0.4745],\n",
            "         [-0.2784, -0.2549, -0.2314,  ..., -0.5059, -0.5294, -0.5451],\n",
            "         [-0.3255, -0.3255, -0.3176,  ..., -0.5294, -0.5529, -0.5686],\n",
            "         ...,\n",
            "         [-0.8588, -0.8588, -0.8588,  ..., -0.0118, -0.0118, -0.0118],\n",
            "         [-0.7961, -0.7961, -0.7961,  ...,  0.0196,  0.0118,  0.0118],\n",
            "         [-0.6941, -0.6941, -0.6941,  ...,  0.1922,  0.1843,  0.1843]],\n",
            "\n",
            "        [[ 0.2706,  0.2784,  0.3020,  ..., -0.0588, -0.0824, -0.0980],\n",
            "         [ 0.0275,  0.0353,  0.0588,  ..., -0.1294, -0.1529, -0.1686],\n",
            "         [-0.0196, -0.0196, -0.0118,  ..., -0.1529, -0.1765, -0.1922],\n",
            "         ...,\n",
            "         [-0.5059, -0.5059, -0.5059,  ...,  0.0118,  0.0118,  0.0118],\n",
            "         [-0.4431, -0.4431, -0.4431,  ...,  0.0431,  0.0353,  0.0353],\n",
            "         [-0.3412, -0.3412, -0.3412,  ...,  0.2157,  0.2078,  0.2078]],\n",
            "\n",
            "        [[ 0.3961,  0.4039,  0.4039,  ...,  0.0980,  0.0745,  0.0588],\n",
            "         [ 0.1608,  0.1608,  0.1765,  ...,  0.0275,  0.0039, -0.0118],\n",
            "         [ 0.1137,  0.1137,  0.1216,  ...,  0.0039, -0.0196, -0.0353],\n",
            "         ...,\n",
            "         [ 0.0196,  0.0196,  0.0196,  ...,  0.0510,  0.0510,  0.0510],\n",
            "         [ 0.0824,  0.0824,  0.0824,  ...,  0.0824,  0.0745,  0.0745],\n",
            "         [ 0.1922,  0.1922,  0.1922,  ...,  0.2549,  0.2471,  0.2471]]])\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCt7iVnqdEG"
      },
      "source": [
        "# 하이퍼 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhSINRr2RITa"
      },
      "source": [
        "num_epochs = 10\n",
        "lr = 1e-4\n",
        "batch_size = 16\n",
        "log_interval = 10"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gql6fhRSrkhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f802112-995c-4e71-c780-630578893bde"
      },
      "source": [
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_loader = data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(len(train_dataset))\n",
        "print(len(validation_dataset))\n",
        "print(len(train_loader))\n",
        "print(len(validation_loader))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8021\n",
            "1296\n",
            "502\n",
            "81\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEEp2bw_09UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd263c60-7686-40d2-e333-b5a053199b34"
      },
      "source": [
        "save_path = drivepath + \"/test_model.pt\"\n",
        "\n",
        "model = KartModel1()\n",
        "# model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "for param, weight in model.named_parameters():\n",
        "    print(f\"param {param:20} required gradient? -> {weight.requires_grad}\")\n",
        "model = model.to(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "param backbone.conv1.weight required gradient? -> True\n",
            "param backbone.bn1.weight  required gradient? -> True\n",
            "param backbone.bn1.bias    required gradient? -> True\n",
            "param backbone.layer1.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer1.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer1.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer1.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer1.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer2.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer2.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer2.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer2.3.conv1.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn1.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn1.bias required gradient? -> True\n",
            "param backbone.layer2.3.conv2.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn2.weight required gradient? -> True\n",
            "param backbone.layer2.3.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer3.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer3.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer3.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.2.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.3.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.3.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.3.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.4.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.4.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.4.bn2.bias required gradient? -> True\n",
            "param backbone.layer3.5.conv1.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn1.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn1.bias required gradient? -> True\n",
            "param backbone.layer3.5.conv2.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn2.weight required gradient? -> True\n",
            "param backbone.layer3.5.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.0.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.0.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.0.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.0.downsample.0.weight required gradient? -> True\n",
            "param backbone.layer4.0.downsample.1.weight required gradient? -> True\n",
            "param backbone.layer4.0.downsample.1.bias required gradient? -> True\n",
            "param backbone.layer4.1.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.1.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.1.bn2.bias required gradient? -> True\n",
            "param backbone.layer4.2.conv1.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn1.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn1.bias required gradient? -> True\n",
            "param backbone.layer4.2.conv2.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn2.weight required gradient? -> True\n",
            "param backbone.layer4.2.bn2.bias required gradient? -> True\n",
            "param backbone.fc.0.weight required gradient? -> True\n",
            "param backbone.fc.0.bias   required gradient? -> True\n",
            "param backbone.fc.1.weight required gradient? -> True\n",
            "param backbone.fc.1.bias   required gradient? -> True\n",
            "param backbone.fc.3.weight required gradient? -> True\n",
            "param backbone.fc.3.bias   required gradient? -> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL90tvKMLKVt"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
        "# scheduler = StepLR(optimizer, 5, gamma=0.5)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=50)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtaaB5sNndlt",
        "outputId": "761a6a8c-6979-4814-8047-2cd6911c11d1"
      },
      "source": [
        "it = iter(train_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "bFQfddsJ2FXn",
        "outputId": "9a0e4400-6ec6-4b51-ae3e-f3d73d9f3871"
      },
      "source": [
        "from torchvision.transforms import ToPILImage\n",
        "from IPython.display import Image\n",
        "to_img = ToPILImage()\n",
        "\n",
        "inputs, labels = next(it)\n",
        "display(to_img(inputs[0]))\n",
        "\n",
        "inputs = inputs.to(device)\n",
        "labels = labels.to(device)\n",
        "outs = model(inputs)\n",
        "preds = torch.argmax(outs, dim=-1)\n",
        "print(preds)\n",
        "print(labels)\n",
        "print((preds == labels).sum())\n",
        "loss = criterion(outs, labels)\n",
        "print(labels.cpu())\n",
        "print(loss.cpu())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FF9101674D0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([7, 7, 6, 7, 2, 0, 7, 0, 0, 3, 0, 0, 5, 7, 0, 0], device='cuda:0')\n",
            "tensor([4, 4, 1, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 6, 5], device='cuda:0')\n",
            "tensor(0, device='cuda:0')\n",
            "tensor([4, 4, 1, 5, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 6, 5])\n",
            "tensor(2.3877, grad_fn=<CopyBackwards>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97MpR5dh-2H7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd96ede4-33a4-4af8-ef01-40b5bdf44720"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.18)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBXF3hbr_CnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835d6f7c-3134-4bd2-bc2c-b58a43a1a463"
      },
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunhyeokk\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywyX1-As_Nou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "8755d494-5353-4592-9cc4-c726b1b55037"
      },
      "source": [
        "config = {}\n",
        "config['n_epochs'] = num_epochs\n",
        "config['batch_size'] = batch_size\n",
        "config['lr'] = lr\n",
        "\n",
        "\n",
        "wandb.init(project=\"boomhill24_12\", config=config)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">avid-bush-5</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/junhyeokk/boomhill24_12\" target=\"_blank\">https://wandb.ai/junhyeokk/boomhill24_12</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/junhyeokk/boomhill24_12/runs/310yu1zr\" target=\"_blank\">https://wandb.ai/junhyeokk/boomhill24_12/runs/310yu1zr</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210826_081856-310yu1zr</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7ff87ea5d350>"
            ],
            "text/html": [
              "<h1>Run(310yu1zr)</h1><iframe src=\"https://wandb.ai/junhyeokk/boomhill24_12/runs/310yu1zr\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfhPU3UcEqJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86690f22-8135-466c-9ee9-5ea04275a13f"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  model.eval()\n",
        "  validation_matches = 0\n",
        "  validation_loss_value = 0\n",
        "  for idx, validation_batch in enumerate(validation_loader):\n",
        "    inputs, labels = validation_batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outs = model(inputs)\n",
        "    preds = torch.argmax(outs, dim=-1)\n",
        "    loss = criterion(outs, labels)\n",
        "\n",
        "    validation_loss_value += loss.item()\n",
        "    validation_matches += (preds == labels).sum().item()\n",
        "\n",
        "  validation_loss = validation_loss_value / batch_size / len(validation_loader)\n",
        "  validation_acc = validation_matches / batch_size / len(validation_loader)\n",
        "  wandb.log({\"validation_loss\" : validation_loss, \"validation_acc\" : validation_acc})\n",
        "  print(\n",
        "      f\"Validation[{epoch + 1}/{num_epochs}] || \",\n",
        "      f\"validation loss {validation_loss:4.4} || validation accuracy {validation_acc:4.2%}\"\n",
        "  )\n",
        "\n",
        "  model.train()\n",
        "  loss_value = 0\n",
        "  matches = 0\n",
        "  \n",
        "  for idx, train_batch in enumerate(train_loader):\n",
        "    inputs, labels = train_batch\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outs = model(inputs)\n",
        "    # preds = (outs > 0.5)\n",
        "    preds = torch.argmax(outs, dim=-1)\n",
        "    loss = criterion(outs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_value += loss.item()\n",
        "    matches += (preds == labels).sum().item()\n",
        "    # matches = ((preds == labels).sum(axis = 1) == 6).sum()\n",
        "\n",
        "    # wandb.log({\"epoch\" : epoch, \"training_loss\" : loss.item(), \"training_acc\" : matches / batch_size})\n",
        "    if (idx + 1) % log_interval == 0:\n",
        "      train_loss = loss_value / batch_size / log_interval\n",
        "      train_acc = matches / batch_size / log_interval\n",
        "      current_lr = scheduler.get_last_lr()\n",
        "      \n",
        "      wandb.log({\"epoch\" : epoch, \"training_loss\" : train_loss, \"training_acc\" : train_acc, \"lr\" : current_lr})\n",
        "      print(\n",
        "          f\"Epoch[{epoch + 1}/{num_epochs}]({idx + 1}/{len(train_loader)}) || \",\n",
        "          f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
        "      )\n",
        "\n",
        "      loss_value = 0\n",
        "      matches = 0\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "model.eval()\n",
        "validation_matches = 0\n",
        "validation_loss_value = 0\n",
        "for idx, validation_batch in enumerate(validation_loader):\n",
        "  inputs, labels = validation_batch\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  outs = model(inputs)\n",
        "  preds = torch.argmax(outs, dim=-1)\n",
        "  loss = criterion(outs, labels)\n",
        "\n",
        "  validation_loss_value += loss.item()\n",
        "  validation_matches += (preds == labels).sum().item()\n",
        "\n",
        "validation_loss = validation_loss_value / batch_size / len(validation_loader)\n",
        "validation_acc = validation_matches / batch_size / len(validation_loader)\n",
        "wandb.log({\"validation_loss\" : validation_loss, \"validation_acc\" : validation_acc})\n",
        "print(\n",
        "    f\"Validation[{epoch + 1}/{num_epochs}] || \",\n",
        "    f\"validation loss {validation_loss:4.4} || validation accuracy {validation_acc:4.2%}\"\n",
        ")\n",
        "\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation[1/10] ||  validation loss 0.1276 || validation accuracy 11.34%\n",
            "Epoch[1/10](10/502) ||  training loss 0.1404 || training accuracy 10.62% || lr [9.221639627510075e-05]\n",
            "Epoch[1/10](20/502) ||  training loss 0.1262 || training accuracy 35.62% || lr [6.840622763423389e-05]\n",
            "Epoch[1/10](30/502) ||  training loss 0.1196 || training accuracy 42.50% || lr [3.756550564175724e-05]\n",
            "Epoch[1/10](40/502) ||  training loss 0.1129 || training accuracy 51.25% || lr [1.1474337861210536e-05]\n",
            "Epoch[1/10](50/502) ||  training loss 0.1151 || training accuracy 46.25% || lr [9.866357858642197e-08]\n",
            "Epoch[1/10](60/502) ||  training loss 0.1204 || training accuracy 49.38% || lr [7.78360372489924e-06]\n",
            "Epoch[1/10](70/502) ||  training loss 0.1015 || training accuracy 65.62% || lr [3.15937723657661e-05]\n",
            "Epoch[1/10](80/502) ||  training loss 0.1091 || training accuracy 58.13% || lr [6.243449435824273e-05]\n",
            "Epoch[1/10](90/502) ||  training loss 0.09364 || training accuracy 63.75% || lr [8.852566213878945e-05]\n",
            "Epoch[1/10](100/502) ||  training loss 0.09247 || training accuracy 66.88% || lr [9.990133642141353e-05]\n",
            "Epoch[1/10](110/502) ||  training loss 0.08675 || training accuracy 68.12% || lr [9.221639627510073e-05]\n",
            "Epoch[1/10](120/502) ||  training loss 0.08352 || training accuracy 68.12% || lr [6.840622763423391e-05]\n",
            "Epoch[1/10](130/502) ||  training loss 0.06982 || training accuracy 74.38% || lr [3.7565505641757235e-05]\n",
            "Epoch[1/10](140/502) ||  training loss 0.07059 || training accuracy 70.62% || lr [1.147433786121053e-05]\n",
            "Epoch[1/10](150/502) ||  training loss 0.07325 || training accuracy 75.00% || lr [9.866357858642196e-08]\n",
            "Epoch[1/10](160/502) ||  training loss 0.07772 || training accuracy 73.12% || lr [7.783603724899249e-06]\n",
            "Epoch[1/10](170/502) ||  training loss 0.07159 || training accuracy 74.38% || lr [3.1593772365766105e-05]\n",
            "Epoch[1/10](180/502) ||  training loss 0.0766 || training accuracy 65.62% || lr [6.243449435824274e-05]\n",
            "Epoch[1/10](190/502) ||  training loss 0.07078 || training accuracy 77.50% || lr [8.85256621387895e-05]\n",
            "Epoch[1/10](200/502) ||  training loss 0.06558 || training accuracy 77.50% || lr [9.990133642141361e-05]\n",
            "Epoch[1/10](210/502) ||  training loss 0.06915 || training accuracy 74.38% || lr [9.221639627510083e-05]\n",
            "Epoch[1/10](220/502) ||  training loss 0.07384 || training accuracy 70.00% || lr [6.840622763423395e-05]\n",
            "Epoch[1/10](230/502) ||  training loss 0.06082 || training accuracy 78.12% || lr [3.7565505641757384e-05]\n",
            "Epoch[1/10](240/502) ||  training loss 0.06082 || training accuracy 74.38% || lr [1.1474337861210613e-05]\n",
            "Epoch[1/10](250/502) ||  training loss 0.06567 || training accuracy 69.38% || lr [9.86635785864277e-08]\n",
            "Epoch[1/10](260/502) ||  training loss 0.0662 || training accuracy 72.50% || lr [7.783603724899242e-06]\n",
            "Epoch[1/10](270/502) ||  training loss 0.06589 || training accuracy 68.75% || lr [3.15937723657661e-05]\n",
            "Epoch[1/10](280/502) ||  training loss 0.05535 || training accuracy 76.25% || lr [6.243449435824276e-05]\n",
            "Epoch[1/10](290/502) ||  training loss 0.05976 || training accuracy 75.62% || lr [8.852566213878955e-05]\n",
            "Epoch[1/10](300/502) ||  training loss 0.06874 || training accuracy 74.38% || lr [9.99013364214137e-05]\n",
            "Epoch[1/10](310/502) ||  training loss 0.05471 || training accuracy 76.88% || lr [9.221639627510087e-05]\n",
            "Epoch[1/10](320/502) ||  training loss 0.06074 || training accuracy 72.50% || lr [6.8406227634234e-05]\n",
            "Epoch[1/10](330/502) ||  training loss 0.04983 || training accuracy 72.50% || lr [3.7565505641757316e-05]\n",
            "Epoch[1/10](340/502) ||  training loss 0.0518 || training accuracy 79.38% || lr [1.1474337861210679e-05]\n",
            "Epoch[1/10](350/502) ||  training loss 0.06074 || training accuracy 72.50% || lr [9.866357858642216e-08]\n",
            "Epoch[1/10](360/502) ||  training loss 0.05206 || training accuracy 76.88% || lr [7.783603724899573e-06]\n",
            "Epoch[1/10](370/502) ||  training loss 0.04544 || training accuracy 80.62% || lr [3.159377236576785e-05]\n",
            "Epoch[1/10](380/502) ||  training loss 0.05168 || training accuracy 76.88% || lr [6.243449435824621e-05]\n",
            "Epoch[1/10](390/502) ||  training loss 0.04541 || training accuracy 78.12% || lr [8.852566213879443e-05]\n",
            "Epoch[1/10](400/502) ||  training loss 0.05157 || training accuracy 73.75% || lr [9.99013364214192e-05]\n",
            "Epoch[1/10](410/502) ||  training loss 0.04373 || training accuracy 79.38% || lr [9.221639627510595e-05]\n",
            "Epoch[1/10](420/502) ||  training loss 0.05671 || training accuracy 75.00% || lr [6.840622763423778e-05]\n",
            "Epoch[1/10](430/502) ||  training loss 0.04726 || training accuracy 75.62% || lr [3.75655056417594e-05]\n",
            "Epoch[1/10](440/502) ||  training loss 0.04643 || training accuracy 72.50% || lr [1.1474337861211202e-05]\n",
            "Epoch[1/10](450/502) ||  training loss 0.05256 || training accuracy 74.38% || lr [9.866357858642758e-08]\n",
            "Epoch[1/10](460/502) ||  training loss 0.04401 || training accuracy 71.25% || lr [7.783603724898788e-06]\n",
            "Epoch[1/10](470/502) ||  training loss 0.04382 || training accuracy 81.25% || lr [3.159377236576413e-05]\n",
            "Epoch[1/10](480/502) ||  training loss 0.05149 || training accuracy 76.25% || lr [6.243449435823923e-05]\n",
            "Epoch[1/10](490/502) ||  training loss 0.0473 || training accuracy 72.50% || lr [8.852566213878442e-05]\n",
            "Epoch[1/10](500/502) ||  training loss 0.04951 || training accuracy 73.12% || lr [9.990133642140804e-05]\n",
            "Validation[2/10] ||  validation loss 0.05803 || validation accuracy 70.91%\n",
            "Epoch[2/10](10/502) ||  training loss 0.0399 || training accuracy 82.50% || lr [8.852566213878461e-05]\n",
            "Epoch[2/10](20/502) ||  training loss 0.04215 || training accuracy 77.50% || lr [6.243449435823919e-05]\n",
            "Epoch[2/10](30/502) ||  training loss 0.03795 || training accuracy 81.25% || lr [3.1593772365764425e-05]\n",
            "Epoch[2/10](40/502) ||  training loss 0.04108 || training accuracy 77.50% || lr [7.783603724898763e-06]\n",
            "Epoch[2/10](50/502) ||  training loss 0.04255 || training accuracy 79.38% || lr [9.866357858642205e-08]\n",
            "Epoch[2/10](60/502) ||  training loss 0.03714 || training accuracy 80.62% || lr [1.1474337861211241e-05]\n",
            "Epoch[2/10](70/502) ||  training loss 0.03956 || training accuracy 78.75% || lr [3.756550564175926e-05]\n",
            "Epoch[2/10](80/502) ||  training loss 0.0407 || training accuracy 80.62% || lr [6.840622763423779e-05]\n",
            "Epoch[2/10](90/502) ||  training loss 0.03343 || training accuracy 84.38% || lr [9.22163962751058e-05]\n",
            "Epoch[2/10](100/502) ||  training loss 0.03654 || training accuracy 81.25% || lr [9.990133642141907e-05]\n",
            "Epoch[2/10](110/502) ||  training loss 0.02931 || training accuracy 87.50% || lr [8.852566213879439e-05]\n",
            "Epoch[2/10](120/502) ||  training loss 0.0353 || training accuracy 82.50% || lr [6.243449435824609e-05]\n",
            "Epoch[2/10](130/502) ||  training loss 0.03793 || training accuracy 80.62% || lr [3.159377236576793e-05]\n",
            "Epoch[2/10](140/502) ||  training loss 0.04932 || training accuracy 71.25% || lr [7.78360372489963e-06]\n",
            "Epoch[2/10](150/502) ||  training loss 0.03959 || training accuracy 80.62% || lr [9.866357858642205e-08]\n",
            "Epoch[2/10](160/502) ||  training loss 0.03482 || training accuracy 81.25% || lr [1.1474337861211236e-05]\n",
            "Epoch[2/10](170/502) ||  training loss 0.03624 || training accuracy 81.88% || lr [3.756550564175896e-05]\n",
            "Epoch[2/10](180/502) ||  training loss 0.03314 || training accuracy 82.50% || lr [6.840622763423756e-05]\n",
            "Epoch[2/10](190/502) ||  training loss 0.0419 || training accuracy 75.62% || lr [9.2216396275106e-05]\n",
            "Epoch[2/10](200/502) ||  training loss 0.039 || training accuracy 80.00% || lr [9.990133642141932e-05]\n",
            "Epoch[2/10](210/502) ||  training loss 0.03588 || training accuracy 78.12% || lr [8.852566213879462e-05]\n",
            "Epoch[2/10](220/502) ||  training loss 0.03684 || training accuracy 78.12% || lr [6.243449435824662e-05]\n",
            "Epoch[2/10](230/502) ||  training loss 0.03699 || training accuracy 80.00% || lr [3.1593772365768036e-05]\n",
            "Epoch[2/10](240/502) ||  training loss 0.03192 || training accuracy 83.75% || lr [7.783603724899659e-06]\n",
            "Epoch[2/10](250/502) ||  training loss 0.03596 || training accuracy 81.88% || lr [9.866357858642205e-08]\n",
            "Epoch[2/10](260/502) ||  training loss 0.041 || training accuracy 80.00% || lr [1.1474337861211002e-05]\n",
            "Epoch[2/10](270/502) ||  training loss 0.03478 || training accuracy 80.00% || lr [3.756550564175927e-05]\n",
            "Epoch[2/10](280/502) ||  training loss 0.03623 || training accuracy 78.12% || lr [6.84062276342378e-05]\n",
            "Epoch[2/10](290/502) ||  training loss 0.04306 || training accuracy 75.62% || lr [9.221639627510584e-05]\n",
            "Epoch[2/10](300/502) ||  training loss 0.0446 || training accuracy 70.62% || lr [9.990133642141913e-05]\n",
            "Epoch[2/10](310/502) ||  training loss 0.04377 || training accuracy 73.12% || lr [8.852566213879445e-05]\n",
            "Epoch[2/10](320/502) ||  training loss 0.03863 || training accuracy 76.88% || lr [6.243449435824648e-05]\n",
            "Epoch[2/10](330/502) ||  training loss 0.04309 || training accuracy 76.25% || lr [3.159377236576797e-05]\n",
            "Epoch[2/10](340/502) ||  training loss 0.0383 || training accuracy 79.38% || lr [7.783603724899645e-06]\n",
            "Epoch[2/10](350/502) ||  training loss 0.03862 || training accuracy 80.00% || lr [9.866357858642205e-08]\n",
            "Epoch[2/10](360/502) ||  training loss 0.03904 || training accuracy 79.38% || lr [1.1474337861210994e-05]\n",
            "Epoch[2/10](370/502) ||  training loss 0.03711 || training accuracy 73.12% || lr [3.756550564175926e-05]\n",
            "Epoch[2/10](380/502) ||  training loss 0.03136 || training accuracy 78.12% || lr [6.840622763423782e-05]\n",
            "Epoch[2/10](390/502) ||  training loss 0.03031 || training accuracy 80.62% || lr [9.22163962751061e-05]\n",
            "Epoch[2/10](400/502) ||  training loss 0.03367 || training accuracy 79.38% || lr [9.990133642141924e-05]\n",
            "Epoch[2/10](410/502) ||  training loss 0.03559 || training accuracy 80.00% || lr [8.852566213879456e-05]\n",
            "Epoch[2/10](420/502) ||  training loss 0.03783 || training accuracy 76.88% || lr [6.243449435824624e-05]\n",
            "Epoch[2/10](430/502) ||  training loss 0.03709 || training accuracy 79.38% || lr [3.159377236576768e-05]\n",
            "Epoch[2/10](440/502) ||  training loss 0.03687 || training accuracy 74.38% || lr [7.783603724899854e-06]\n",
            "Epoch[2/10](450/502) ||  training loss 0.03235 || training accuracy 80.62% || lr [9.866357858642205e-08]\n",
            "Epoch[2/10](460/502) ||  training loss 0.03587 || training accuracy 75.00% || lr [1.1474337861211856e-05]\n",
            "Epoch[2/10](470/502) ||  training loss 0.03713 || training accuracy 73.12% || lr [3.7565505641761694e-05]\n",
            "Epoch[2/10](480/502) ||  training loss 0.03451 || training accuracy 80.00% || lr [6.840622763424127e-05]\n",
            "Epoch[2/10](490/502) ||  training loss 0.03202 || training accuracy 81.25% || lr [9.221639627511099e-05]\n",
            "Epoch[2/10](500/502) ||  training loss 0.03904 || training accuracy 78.75% || lr [9.990133642142477e-05]\n",
            "Validation[3/10] ||  validation loss 0.05021 || validation accuracy 71.68%\n",
            "Epoch[3/10](10/502) ||  training loss 0.03068 || training accuracy 82.50% || lr [8.422735529644402e-05]\n",
            "Epoch[3/10](20/502) ||  training loss 0.0349 || training accuracy 81.88% || lr [5.626666167822151e-05]\n",
            "Epoch[3/10](30/502) ||  training loss 0.03016 || training accuracy 81.25% || lr [2.5912316294917604e-05]\n",
            "Epoch[3/10](40/502) ||  training loss 0.03397 || training accuracy 78.75% || lr [4.75864737669971e-06]\n",
            "Epoch[3/10](50/502) ||  training loss 0.02805 || training accuracy 85.62% || lr [8.856374635658295e-07]\n",
            "Epoch[3/10](60/502) ||  training loss 0.02495 || training accuracy 87.50% || lr [1.577264470357089e-05]\n",
            "Epoch[3/10](70/502) ||  training loss 0.0338 || training accuracy 78.12% || lr [4.3733338321799025e-05]\n",
            "Epoch[3/10](80/502) ||  training loss 0.03059 || training accuracy 85.00% || lr [7.408768370511045e-05]\n",
            "Epoch[3/10](90/502) ||  training loss 0.03198 || training accuracy 82.50% || lr [9.524135262333302e-05]\n",
            "Epoch[3/10](100/502) ||  training loss 0.03389 || training accuracy 81.88% || lr [9.911436253646789e-05]\n",
            "Epoch[3/10](110/502) ||  training loss 0.03097 || training accuracy 81.88% || lr [8.422735529646326e-05]\n",
            "Epoch[3/10](120/502) ||  training loss 0.03441 || training accuracy 79.38% || lr [5.6266661678234576e-05]\n",
            "Epoch[3/10](130/502) ||  training loss 0.03448 || training accuracy 77.50% || lr [2.591231629492315e-05]\n",
            "Epoch[3/10](140/502) ||  training loss 0.03023 || training accuracy 82.50% || lr [4.758647376700636e-06]\n",
            "Epoch[3/10](150/502) ||  training loss 0.03296 || training accuracy 78.75% || lr [8.856374635655637e-07]\n",
            "Epoch[3/10](160/502) ||  training loss 0.03093 || training accuracy 81.88% || lr [1.5772644703567066e-05]\n",
            "Epoch[3/10](170/502) ||  training loss 0.02943 || training accuracy 85.00% || lr [4.3733338321789525e-05]\n",
            "Epoch[3/10](180/502) ||  training loss 0.02898 || training accuracy 82.50% || lr [7.40876837050941e-05]\n",
            "Epoch[3/10](190/502) ||  training loss 0.02096 || training accuracy 88.12% || lr [9.524135262331146e-05]\n",
            "Epoch[3/10](200/502) ||  training loss 0.03081 || training accuracy 81.25% || lr [9.911436253644566e-05]\n",
            "Epoch[3/10](210/502) ||  training loss 0.03002 || training accuracy 78.75% || lr [8.422735529644406e-05]\n",
            "Epoch[3/10](220/502) ||  training loss 0.02946 || training accuracy 82.50% || lr [5.6266661678221586e-05]\n",
            "Epoch[3/10](230/502) ||  training loss 0.03302 || training accuracy 81.25% || lr [2.591231629491765e-05]\n",
            "Epoch[3/10](240/502) ||  training loss 0.03021 || training accuracy 80.00% || lr [4.758647376699723e-06]\n",
            "Epoch[3/10](250/502) ||  training loss 0.02707 || training accuracy 85.62% || lr [8.856374635658239e-07]\n",
            "Epoch[3/10](260/502) ||  training loss 0.03359 || training accuracy 81.88% || lr [1.5772644703570868e-05]\n",
            "Epoch[3/10](270/502) ||  training loss 0.03419 || training accuracy 80.00% || lr [4.373333832179971e-05]\n",
            "Epoch[3/10](280/502) ||  training loss 0.03081 || training accuracy 82.50% || lr [7.408768370511048e-05]\n",
            "Epoch[3/10](290/502) ||  training loss 0.03154 || training accuracy 82.50% || lr [9.524135262333309e-05]\n",
            "Epoch[3/10](300/502) ||  training loss 0.03298 || training accuracy 81.88% || lr [9.911436253646793e-05]\n",
            "Epoch[3/10](310/502) ||  training loss 0.03164 || training accuracy 80.62% || lr [8.422735529646279e-05]\n",
            "Epoch[3/10](320/502) ||  training loss 0.03345 || training accuracy 78.75% || lr [5.626666167823461e-05]\n",
            "Epoch[3/10](330/502) ||  training loss 0.03228 || training accuracy 81.88% || lr [2.5912316294923184e-05]\n",
            "Epoch[3/10](340/502) ||  training loss 0.03414 || training accuracy 81.88% || lr [4.758647376700652e-06]\n",
            "Epoch[3/10](350/502) ||  training loss 0.02912 || training accuracy 76.88% || lr [8.856374635655581e-07]\n",
            "Epoch[3/10](360/502) ||  training loss 0.03238 || training accuracy 80.62% || lr [1.5772644703567574e-05]\n",
            "Epoch[3/10](370/502) ||  training loss 0.03177 || training accuracy 83.12% || lr [4.3733338321789464e-05]\n",
            "Epoch[3/10](380/502) ||  training loss 0.03268 || training accuracy 81.25% || lr [7.408768370509403e-05]\n",
            "Epoch[3/10](390/502) ||  training loss 0.03552 || training accuracy 79.38% || lr [9.52413526233117e-05]\n",
            "Epoch[3/10](400/502) ||  training loss 0.03151 || training accuracy 81.25% || lr [9.91143625364456e-05]\n",
            "Epoch[3/10](410/502) ||  training loss 0.02984 || training accuracy 78.12% || lr [8.4227355296444e-05]\n",
            "Epoch[3/10](420/502) ||  training loss 0.02755 || training accuracy 81.88% || lr [5.6266661678221566e-05]\n",
            "Epoch[3/10](430/502) ||  training loss 0.02851 || training accuracy 80.62% || lr [2.591231629491765e-05]\n",
            "Epoch[3/10](440/502) ||  training loss 0.03268 || training accuracy 81.25% || lr [4.758647376699426e-06]\n",
            "Epoch[3/10](450/502) ||  training loss 0.03553 || training accuracy 79.38% || lr [8.856374635654255e-07]\n",
            "Epoch[3/10](460/502) ||  training loss 0.02994 || training accuracy 80.00% || lr [1.577264470356375e-05]\n",
            "Epoch[3/10](470/502) ||  training loss 0.02209 || training accuracy 86.88% || lr [4.373333832177998e-05]\n",
            "Epoch[3/10](480/502) ||  training loss 0.03258 || training accuracy 80.62% || lr [7.408768370507706e-05]\n",
            "Epoch[3/10](490/502) ||  training loss 0.03683 || training accuracy 78.12% || lr [9.524135262329045e-05]\n",
            "Epoch[3/10](500/502) ||  training loss 0.03697 || training accuracy 76.25% || lr [9.911436253642323e-05]\n",
            "Validation[4/10] ||  validation loss 0.04578 || validation accuracy 73.61%\n",
            "Epoch[4/10](10/502) ||  training loss 0.02895 || training accuracy 84.38% || lr [7.938926261461434e-05]\n",
            "Epoch[4/10](20/502) ||  training loss 0.02522 || training accuracy 86.88% || lr [4.999999999999443e-05]\n",
            "Epoch[4/10](30/502) ||  training loss 0.03047 || training accuracy 81.88% || lr [2.0610737385374516e-05]\n",
            "Epoch[4/10](40/502) ||  training loss 0.02702 || training accuracy 83.12% || lr [2.4471741852419625e-06]\n",
            "Epoch[4/10](50/502) ||  training loss 0.02683 || training accuracy 85.00% || lr [2.447174185241409e-06]\n",
            "Epoch[4/10](60/502) ||  training loss 0.02173 || training accuracy 85.00% || lr [2.061073738536874e-05]\n",
            "Epoch[4/10](70/502) ||  training loss 0.02944 || training accuracy 78.75% || lr [4.9999999999983224e-05]\n",
            "Epoch[4/10](80/502) ||  training loss 0.02926 || training accuracy 82.50% || lr [7.938926261459654e-05]\n",
            "Epoch[4/10](90/502) ||  training loss 0.02476 || training accuracy 85.00% || lr [9.755282581472496e-05]\n",
            "Epoch[4/10](100/502) ||  training loss 0.03002 || training accuracy 82.50% || lr [9.755282581472482e-05]\n",
            "Epoch[4/10](110/502) ||  training loss 0.02578 || training accuracy 84.38% || lr [7.938926261459741e-05]\n",
            "Epoch[4/10](120/502) ||  training loss 0.02555 || training accuracy 81.88% || lr [4.9999999999982886e-05]\n",
            "Epoch[4/10](130/502) ||  training loss 0.02824 || training accuracy 83.75% || lr [2.061073738536962e-05]\n",
            "Epoch[4/10](140/502) ||  training loss 0.02405 || training accuracy 86.25% || lr [2.447174185241747e-06]\n",
            "Epoch[4/10](150/502) ||  training loss 0.02777 || training accuracy 85.00% || lr [2.447174185243304e-06]\n",
            "Epoch[4/10](160/502) ||  training loss 0.02194 || training accuracy 86.25% || lr [2.061073738538408e-05]\n",
            "Epoch[4/10](170/502) ||  training loss 0.02387 || training accuracy 85.00% || lr [5.0000000000020114e-05]\n",
            "Epoch[4/10](180/502) ||  training loss 0.02326 || training accuracy 82.50% || lr [7.938926261465483e-05]\n",
            "Epoch[4/10](190/502) ||  training loss 0.02767 || training accuracy 83.12% || lr [9.75528258147959e-05]\n",
            "Epoch[4/10](200/502) ||  training loss 0.03054 || training accuracy 80.00% || lr [9.755282581479601e-05]\n",
            "Epoch[4/10](210/502) ||  training loss 0.02698 || training accuracy 85.00% || lr [7.938926261465511e-05]\n",
            "Epoch[4/10](220/502) ||  training loss 0.03116 || training accuracy 78.12% || lr [5.00000000000205e-05]\n",
            "Epoch[4/10](230/502) ||  training loss 0.02995 || training accuracy 82.50% || lr [2.0610737385384406e-05]\n",
            "Epoch[4/10](240/502) ||  training loss 0.03148 || training accuracy 79.38% || lr [2.4471741852434312e-06]\n",
            "Epoch[4/10](250/502) ||  training loss 0.03253 || training accuracy 81.25% || lr [2.447174185242642e-06]\n",
            "Epoch[4/10](260/502) ||  training loss 0.02958 || training accuracy 80.62% || lr [2.061073738537972e-05]\n",
            "Epoch[4/10](270/502) ||  training loss 0.02786 || training accuracy 82.50% || lr [5.0000000000007775e-05]\n",
            "Epoch[4/10](280/502) ||  training loss 0.02748 || training accuracy 83.12% || lr [7.938926261463727e-05]\n",
            "Epoch[4/10](290/502) ||  training loss 0.0295 || training accuracy 81.88% || lr [9.755282581477407e-05]\n",
            "Epoch[4/10](300/502) ||  training loss 0.02815 || training accuracy 81.88% || lr [9.75528258147742e-05]\n",
            "Epoch[4/10](310/502) ||  training loss 0.03401 || training accuracy 78.75% || lr [7.938926261463704e-05]\n",
            "Epoch[4/10](320/502) ||  training loss 0.02991 || training accuracy 83.12% || lr [5.000000000000895e-05]\n",
            "Epoch[4/10](330/502) ||  training loss 0.02645 || training accuracy 81.88% || lr [2.061073738538008e-05]\n",
            "Epoch[4/10](340/502) ||  training loss 0.03147 || training accuracy 78.75% || lr [2.447174185242775e-06]\n",
            "Epoch[4/10](350/502) ||  training loss 0.02366 || training accuracy 85.62% || lr [2.4471741852419743e-06]\n",
            "Epoch[4/10](360/502) ||  training loss 0.02409 || training accuracy 85.00% || lr [2.061073738537478e-05]\n",
            "Epoch[4/10](370/502) ||  training loss 0.02689 || training accuracy 80.62% || lr [4.9999999999996845e-05]\n",
            "Epoch[4/10](380/502) ||  training loss 0.0208 || training accuracy 88.75% || lr [7.938926261461902e-05]\n",
            "Epoch[4/10](390/502) ||  training loss 0.03435 || training accuracy 77.50% || lr [9.755282581475213e-05]\n",
            "Epoch[4/10](400/502) ||  training loss 0.02531 || training accuracy 84.38% || lr [9.755282581475232e-05]\n",
            "Epoch[4/10](410/502) ||  training loss 0.02341 || training accuracy 86.25% || lr [7.938926261461942e-05]\n",
            "Epoch[4/10](420/502) ||  training loss 0.03101 || training accuracy 80.00% || lr [4.9999999999997305e-05]\n",
            "Epoch[4/10](430/502) ||  training loss 0.02889 || training accuracy 85.00% || lr [2.0610737385375156e-05]\n",
            "Epoch[4/10](440/502) ||  training loss 0.03769 || training accuracy 80.00% || lr [2.4471741852423407e-06]\n",
            "Epoch[4/10](450/502) ||  training loss 0.0224 || training accuracy 88.12% || lr [2.4471741852426312e-06]\n",
            "Epoch[4/10](460/502) ||  training loss 0.0274 || training accuracy 81.88% || lr [2.06107373853797e-05]\n",
            "Epoch[4/10](470/502) ||  training loss 0.02779 || training accuracy 83.12% || lr [5.000000000000845e-05]\n",
            "Epoch[4/10](480/502) ||  training loss 0.02732 || training accuracy 81.88% || lr [7.938926261463723e-05]\n",
            "Epoch[4/10](490/502) ||  training loss 0.02343 || training accuracy 85.62% || lr [9.755282581477406e-05]\n",
            "Epoch[4/10](500/502) ||  training loss 0.02703 || training accuracy 82.50% || lr [9.755282581477418e-05]\n",
            "Validation[5/10] ||  validation loss 0.04587 || validation accuracy 71.99%\n",
            "Epoch[5/10](10/502) ||  training loss 0.02474 || training accuracy 86.88% || lr [7.408768370509856e-05]\n",
            "Epoch[5/10](20/502) ||  training loss 0.02911 || training accuracy 83.75% || lr [4.373333832179234e-05]\n",
            "Epoch[5/10](30/502) ||  training loss 0.02483 || training accuracy 88.12% || lr [1.577264470356824e-05]\n",
            "Epoch[5/10](40/502) ||  training loss 0.02363 || training accuracy 83.75% || lr [8.856374635659519e-07]\n",
            "Epoch[5/10](50/502) ||  training loss 0.02982 || training accuracy 82.50% || lr [4.75864737670104e-06]\n",
            "Epoch[5/10](60/502) ||  training loss 0.02408 || training accuracy 84.38% || lr [2.5912316294923638e-05]\n",
            "Epoch[5/10](70/502) ||  training loss 0.02372 || training accuracy 86.25% || lr [5.626666167823662e-05]\n",
            "Epoch[5/10](80/502) ||  training loss 0.0253 || training accuracy 83.75% || lr [8.422735529646718e-05]\n",
            "Epoch[5/10](90/502) ||  training loss 0.02402 || training accuracy 85.00% || lr [9.911436253647338e-05]\n",
            "Epoch[5/10](100/502) ||  training loss 0.02526 || training accuracy 81.88% || lr [9.524135262333856e-05]\n",
            "Epoch[5/10](110/502) ||  training loss 0.02045 || training accuracy 87.50% || lr [7.408768370511494e-05]\n",
            "Epoch[5/10](120/502) ||  training loss 0.028 || training accuracy 81.25% || lr [4.373333832180186e-05]\n",
            "Epoch[5/10](130/502) ||  training loss 0.02447 || training accuracy 83.75% || lr [1.5772644703571535e-05]\n",
            "Epoch[5/10](140/502) ||  training loss 0.02252 || training accuracy 85.00% || lr [8.856374635660847e-07]\n",
            "Epoch[5/10](150/502) ||  training loss 0.02696 || training accuracy 83.12% || lr [4.758647376699508e-06]\n",
            "Epoch[5/10](160/502) ||  training loss 0.02331 || training accuracy 82.50% || lr [2.591231629491812e-05]\n",
            "Epoch[5/10](170/502) ||  training loss 0.02637 || training accuracy 83.75% || lr [5.626666167822432e-05]\n",
            "Epoch[5/10](180/502) ||  training loss 0.02695 || training accuracy 81.25% || lr [8.422735529644849e-05]\n",
            "Epoch[5/10](190/502) ||  training loss 0.0225 || training accuracy 88.75% || lr [9.911436253645111e-05]\n",
            "Epoch[5/10](200/502) ||  training loss 0.02868 || training accuracy 85.00% || lr [9.524135262331694e-05]\n",
            "Epoch[5/10](210/502) ||  training loss 0.01959 || training accuracy 88.12% || lr [7.408768370509793e-05]\n",
            "Epoch[5/10](220/502) ||  training loss 0.02689 || training accuracy 81.25% || lr [4.373333832179306e-05]\n",
            "Epoch[5/10](230/502) ||  training loss 0.02534 || training accuracy 84.38% || lr [1.5772644703568767e-05]\n",
            "Epoch[5/10](240/502) ||  training loss 0.02626 || training accuracy 85.00% || lr [8.856374635658238e-07]\n",
            "Epoch[5/10](250/502) ||  training loss 0.01937 || training accuracy 88.12% || lr [4.758647376698588e-06]\n",
            "Epoch[5/10](260/502) ||  training loss 0.01875 || training accuracy 87.50% || lr [2.591231629491259e-05]\n",
            "Epoch[5/10](270/502) ||  training loss 0.03031 || training accuracy 81.25% || lr [5.6266661678212025e-05]\n",
            "Epoch[5/10](280/502) ||  training loss 0.02168 || training accuracy 85.62% || lr [8.42273552964298e-05]\n",
            "Epoch[5/10](290/502) ||  training loss 0.02742 || training accuracy 85.00% || lr [9.911436253642891e-05]\n",
            "Epoch[5/10](300/502) ||  training loss 0.02627 || training accuracy 82.50% || lr [9.524135262329542e-05]\n",
            "Epoch[5/10](310/502) ||  training loss 0.02759 || training accuracy 85.62% || lr [7.408768370508228e-05]\n",
            "Epoch[5/10](320/502) ||  training loss 0.02385 || training accuracy 83.75% || lr [4.373333832178292e-05]\n",
            "Epoch[5/10](330/502) ||  training loss 0.02904 || training accuracy 78.75% || lr [1.5772644703564982e-05]\n",
            "Epoch[5/10](340/502) ||  training loss 0.02477 || training accuracy 85.00% || lr [8.856374635655588e-07]\n",
            "Epoch[5/10](350/502) ||  training loss 0.02421 || training accuracy 83.12% || lr [4.758647376697661e-06]\n",
            "Epoch[5/10](360/502) ||  training loss 0.02381 || training accuracy 87.50% || lr [2.5912316294907063e-05]\n",
            "Epoch[5/10](370/502) ||  training loss 0.01937 || training accuracy 90.00% || lr [5.626666167819827e-05]\n",
            "Epoch[5/10](380/502) ||  training loss 0.02346 || training accuracy 86.25% || lr [8.42273552964111e-05]\n",
            "Epoch[5/10](390/502) ||  training loss 0.02246 || training accuracy 85.00% || lr [9.91143625364064e-05]\n",
            "Epoch[5/10](400/502) ||  training loss 0.02001 || training accuracy 88.12% || lr [9.524135262327443e-05]\n",
            "Epoch[5/10](410/502) ||  training loss 0.02313 || training accuracy 87.50% || lr [7.408768370506531e-05]\n",
            "Epoch[5/10](420/502) ||  training loss 0.02625 || training accuracy 82.50% || lr [4.373333832177274e-05]\n",
            "Epoch[5/10](430/502) ||  training loss 0.02264 || training accuracy 88.75% || lr [1.577264470356118e-05]\n",
            "Epoch[5/10](440/502) ||  training loss 0.02296 || training accuracy 85.62% || lr [8.856374635652983e-07]\n",
            "Epoch[5/10](450/502) ||  training loss 0.02015 || training accuracy 88.75% || lr [4.758647376696396e-06]\n",
            "Epoch[5/10](460/502) ||  training loss 0.02341 || training accuracy 86.88% || lr [2.5912316294902977e-05]\n",
            "Epoch[5/10](470/502) ||  training loss 0.02613 || training accuracy 83.12% || lr [5.626666167818913e-05]\n",
            "Epoch[5/10](480/502) ||  training loss 0.0208 || training accuracy 89.38% || lr [8.422735529639713e-05]\n",
            "Epoch[5/10](490/502) ||  training loss 0.02119 || training accuracy 85.00% || lr [9.911436253638972e-05]\n",
            "Epoch[5/10](500/502) ||  training loss 0.02201 || training accuracy 87.50% || lr [9.524135262325821e-05]\n",
            "Validation[6/10] ||  validation loss 0.04799 || validation accuracy 69.37%\n",
            "Epoch[6/10](10/502) ||  training loss 0.02248 || training accuracy 87.50% || lr [6.840622763420288e-05]\n",
            "Epoch[6/10](20/502) ||  training loss 0.02133 || training accuracy 85.62% || lr [3.756550564174132e-05]\n",
            "Epoch[6/10](30/502) ||  training loss 0.02072 || training accuracy 86.88% || lr [1.1474337861204997e-05]\n",
            "Epoch[6/10](40/502) ||  training loss 0.02003 || training accuracy 89.38% || lr [9.866357858641652e-08]\n",
            "Epoch[6/10](50/502) ||  training loss 0.02138 || training accuracy 84.38% || lr [7.783603724900743e-06]\n",
            "Epoch[6/10](60/502) ||  training loss 0.01505 || training accuracy 90.62% || lr [3.1593772365772936e-05]\n",
            "Epoch[6/10](70/502) ||  training loss 0.01358 || training accuracy 91.88% || lr [6.243449435825665e-05]\n",
            "Epoch[6/10](80/502) ||  training loss 0.01697 || training accuracy 91.88% || lr [8.852566213880846e-05]\n",
            "Epoch[6/10](90/502) ||  training loss 0.01809 || training accuracy 88.12% || lr [9.990133642143604e-05]\n",
            "Epoch[6/10](100/502) ||  training loss 0.01503 || training accuracy 90.62% || lr [9.221639627512201e-05]\n",
            "Epoch[6/10](110/502) ||  training loss 0.03286 || training accuracy 82.50% || lr [6.840622763425003e-05]\n",
            "Epoch[6/10](120/502) ||  training loss 0.02028 || training accuracy 88.75% || lr [3.7565505641766335e-05]\n",
            "Epoch[6/10](130/502) ||  training loss 0.02083 || training accuracy 86.25% || lr [1.1474337861212512e-05]\n",
            "Epoch[6/10](140/502) ||  training loss 0.02143 || training accuracy 86.88% || lr [9.866357858646082e-08]\n",
            "Epoch[6/10](150/502) ||  training loss 0.01697 || training accuracy 89.38% || lr [7.783603724906181e-06]\n",
            "Epoch[6/10](160/502) ||  training loss 0.02017 || training accuracy 89.38% || lr [3.1593772365793285e-05]\n",
            "Epoch[6/10](170/502) ||  training loss 0.01557 || training accuracy 91.25% || lr [6.24344943582978e-05]\n",
            "Epoch[6/10](180/502) ||  training loss 0.01717 || training accuracy 91.88% || lr [8.852566213886945e-05]\n",
            "Epoch[6/10](190/502) ||  training loss 0.02019 || training accuracy 88.12% || lr [9.990133642150359e-05]\n",
            "Epoch[6/10](200/502) ||  training loss 0.02116 || training accuracy 90.00% || lr [9.221639627518416e-05]\n",
            "Epoch[6/10](210/502) ||  training loss 0.02008 || training accuracy 86.88% || lr [6.840622763429597e-05]\n",
            "Epoch[6/10](220/502) ||  training loss 0.02557 || training accuracy 83.75% || lr [3.756550564179141e-05]\n",
            "Epoch[6/10](230/502) ||  training loss 0.02311 || training accuracy 82.50% || lr [1.1474337861221868e-05]\n",
            "Epoch[6/10](240/502) ||  training loss 0.01955 || training accuracy 88.75% || lr [9.866357858651092e-08]\n",
            "Epoch[6/10](250/502) ||  training loss 0.02229 || training accuracy 84.38% || lr [7.783603724904616e-06]\n",
            "Epoch[6/10](260/502) ||  training loss 0.02104 || training accuracy 86.88% || lr [3.1593772365786475e-05]\n",
            "Epoch[6/10](270/502) ||  training loss 0.01981 || training accuracy 86.25% || lr [6.243449435828408e-05]\n",
            "Epoch[6/10](280/502) ||  training loss 0.01627 || training accuracy 90.62% || lr [8.852566213884885e-05]\n",
            "Epoch[6/10](290/502) ||  training loss 0.02771 || training accuracy 81.25% || lr [9.990133642148117e-05]\n",
            "Epoch[6/10](300/502) ||  training loss 0.02294 || training accuracy 88.12% || lr [9.221639627516329e-05]\n",
            "Epoch[6/10](310/502) ||  training loss 0.02403 || training accuracy 85.62% || lr [6.84062276342803e-05]\n",
            "Epoch[6/10](320/502) ||  training loss 0.02301 || training accuracy 85.62% || lr [3.7565505641782646e-05]\n",
            "Epoch[6/10](330/502) ||  training loss 0.02713 || training accuracy 83.12% || lr [1.1474337861219074e-05]\n",
            "Epoch[6/10](340/502) ||  training loss 0.01901 || training accuracy 90.00% || lr [9.866357858646657e-08]\n",
            "Epoch[6/10](350/502) ||  training loss 0.01854 || training accuracy 90.00% || lr [7.783603724903049e-06]\n",
            "Epoch[6/10](360/502) ||  training loss 0.01783 || training accuracy 88.12% || lr [3.1593772365779685e-05]\n",
            "Epoch[6/10](370/502) ||  training loss 0.01822 || training accuracy 88.75% || lr [6.243449435827035e-05]\n",
            "Epoch[6/10](380/502) ||  training loss 0.02336 || training accuracy 83.75% || lr [8.852566213882913e-05]\n",
            "Epoch[6/10](390/502) ||  training loss 0.02061 || training accuracy 88.75% || lr [9.99013364214585e-05]\n",
            "Epoch[6/10](400/502) ||  training loss 0.01939 || training accuracy 85.62% || lr [9.22163962751423e-05]\n",
            "Epoch[6/10](410/502) ||  training loss 0.02018 || training accuracy 86.25% || lr [6.840622763426454e-05]\n",
            "Epoch[6/10](420/502) ||  training loss 0.02518 || training accuracy 82.50% || lr [3.756550564177384e-05]\n",
            "Epoch[6/10](430/502) ||  training loss 0.02091 || training accuracy 86.88% || lr [1.1474337861216267e-05]\n",
            "Epoch[6/10](440/502) ||  training loss 0.01643 || training accuracy 91.88% || lr [9.866357858651094e-08]\n",
            "Epoch[6/10](450/502) ||  training loss 0.01673 || training accuracy 91.88% || lr [7.783603724901478e-06]\n",
            "Epoch[6/10](460/502) ||  training loss 0.01866 || training accuracy 88.12% || lr [3.159377236577289e-05]\n",
            "Epoch[6/10](470/502) ||  training loss 0.02361 || training accuracy 86.25% || lr [6.243449435825662e-05]\n",
            "Epoch[6/10](480/502) ||  training loss 0.01689 || training accuracy 90.62% || lr [8.852566213880941e-05]\n",
            "Epoch[6/10](490/502) ||  training loss 0.01601 || training accuracy 90.62% || lr [9.990133642143602e-05]\n",
            "Epoch[6/10](500/502) ||  training loss 0.01225 || training accuracy 91.88% || lr [9.221639627512209e-05]\n",
            "Validation[7/10] ||  validation loss 0.04664 || validation accuracy 76.16%\n",
            "Epoch[7/10](10/502) ||  training loss 0.01763 || training accuracy 89.38% || lr [6.243449435825737e-05]\n",
            "Epoch[7/10](20/502) ||  training loss 0.02115 || training accuracy 84.38% || lr [3.1593772365774914e-05]\n",
            "Epoch[7/10](30/502) ||  training loss 0.01895 || training accuracy 85.62% || lr [7.783603724901112e-06]\n",
            "Epoch[7/10](40/502) ||  training loss 0.01355 || training accuracy 93.12% || lr [9.866357858642205e-08]\n",
            "Epoch[7/10](50/502) ||  training loss 0.01178 || training accuracy 94.38% || lr [1.1474337861210623e-05]\n",
            "Epoch[7/10](60/502) ||  training loss 0.01809 || training accuracy 90.62% || lr [3.7565505641756185e-05]\n",
            "Epoch[7/10](70/502) ||  training loss 0.01159 || training accuracy 93.75% || lr [6.840622763423302e-05]\n",
            "Epoch[7/10](80/502) ||  training loss 0.01308 || training accuracy 93.12% || lr [9.221639627510107e-05]\n",
            "Epoch[7/10](90/502) ||  training loss 0.01642 || training accuracy 90.00% || lr [9.990133642141357e-05]\n",
            "Epoch[7/10](100/502) ||  training loss 0.01657 || training accuracy 89.38% || lr [8.852566213878966e-05]\n",
            "Epoch[7/10](110/502) ||  training loss 0.01565 || training accuracy 91.25% || lr [6.243449435824292e-05]\n",
            "Epoch[7/10](120/502) ||  training loss 0.01662 || training accuracy 89.38% || lr [3.159377236576746e-05]\n",
            "Epoch[7/10](130/502) ||  training loss 0.01037 || training accuracy 94.38% || lr [7.783603724899937e-06]\n",
            "Epoch[7/10](140/502) ||  training loss 0.01304 || training accuracy 93.12% || lr [9.866357858642205e-08]\n",
            "Epoch[7/10](150/502) ||  training loss 0.01744 || training accuracy 88.75% || lr [1.1474337861208913e-05]\n",
            "Epoch[7/10](160/502) ||  training loss 0.01378 || training accuracy 88.12% || lr [3.7565505641750194e-05]\n",
            "Epoch[7/10](170/502) ||  training loss 0.01209 || training accuracy 93.12% || lr [6.840622763422184e-05]\n",
            "Epoch[7/10](180/502) ||  training loss 0.01113 || training accuracy 94.38% || lr [9.2216396275085e-05]\n",
            "Epoch[7/10](190/502) ||  training loss 0.01726 || training accuracy 90.62% || lr [9.990133642139675e-05]\n",
            "Epoch[7/10](200/502) ||  training loss 0.01851 || training accuracy 88.75% || lr [8.852566213877457e-05]\n",
            "Epoch[7/10](210/502) ||  training loss 0.02 || training accuracy 86.88% || lr [6.243449435823212e-05]\n",
            "Epoch[7/10](220/502) ||  training loss 0.01402 || training accuracy 92.50% || lr [3.159377236576183e-05]\n",
            "Epoch[7/10](230/502) ||  training loss 0.02082 || training accuracy 89.38% || lr [7.78360372489844e-06]\n",
            "Epoch[7/10](240/502) ||  training loss 0.01886 || training accuracy 87.50% || lr [9.866357858642205e-08]\n",
            "Epoch[7/10](250/502) ||  training loss 0.01554 || training accuracy 93.12% || lr [1.1474337861216874e-05]\n",
            "Epoch[7/10](260/502) ||  training loss 0.01786 || training accuracy 90.00% || lr [3.756550564177587e-05]\n",
            "Epoch[7/10](270/502) ||  training loss 0.01354 || training accuracy 91.88% || lr [6.840622763426831e-05]\n",
            "Epoch[7/10](280/502) ||  training loss 0.01354 || training accuracy 91.88% || lr [9.221639627514663e-05]\n",
            "Epoch[7/10](290/502) ||  training loss 0.01501 || training accuracy 90.62% || lr [9.99013364214642e-05]\n",
            "Epoch[7/10](300/502) ||  training loss 0.01983 || training accuracy 88.75% || lr [8.852566213883407e-05]\n",
            "Epoch[7/10](310/502) ||  training loss 0.01978 || training accuracy 85.00% || lr [6.24344943582739e-05]\n",
            "Epoch[7/10](320/502) ||  training loss 0.01752 || training accuracy 91.25% || lr [3.1593772365782815e-05]\n",
            "Epoch[7/10](330/502) ||  training loss 0.01627 || training accuracy 87.50% || lr [7.783603724903504e-06]\n",
            "Epoch[7/10](340/502) ||  training loss 0.01574 || training accuracy 91.88% || lr [9.866357858642205e-08]\n",
            "Epoch[7/10](350/502) ||  training loss 0.02212 || training accuracy 89.38% || lr [1.147433786121451e-05]\n",
            "Epoch[7/10](360/502) ||  training loss 0.02152 || training accuracy 88.75% || lr [3.756550564176777e-05]\n",
            "Epoch[7/10](370/502) ||  training loss 0.02068 || training accuracy 90.00% || lr [6.840622763425328e-05]\n",
            "Epoch[7/10](380/502) ||  training loss 0.01637 || training accuracy 92.50% || lr [9.221639627512613e-05]\n",
            "Epoch[7/10](390/502) ||  training loss 0.0159 || training accuracy 91.88% || lr [9.990133642144179e-05]\n",
            "Epoch[7/10](400/502) ||  training loss 0.0186 || training accuracy 86.88% || lr [8.85256621388149e-05]\n",
            "Epoch[7/10](410/502) ||  training loss 0.01849 || training accuracy 90.62% || lr [6.243449435825956e-05]\n",
            "Epoch[7/10](420/502) ||  training loss 0.02175 || training accuracy 90.00% || lr [3.159377236577543e-05]\n",
            "Epoch[7/10](430/502) ||  training loss 0.01747 || training accuracy 90.62% || lr [7.783603724901583e-06]\n",
            "Epoch[7/10](440/502) ||  training loss 0.01626 || training accuracy 92.50% || lr [9.866357858642205e-08]\n",
            "Epoch[7/10](450/502) ||  training loss 0.0213 || training accuracy 89.38% || lr [1.147433786121034e-05]\n",
            "Epoch[7/10](460/502) ||  training loss 0.0174 || training accuracy 89.38% || lr [3.756550564175963e-05]\n",
            "Epoch[7/10](470/502) ||  training loss 0.01408 || training accuracy 90.62% || lr [6.840622763423813e-05]\n",
            "Epoch[7/10](480/502) ||  training loss 0.02302 || training accuracy 83.75% || lr [9.221639627510548e-05]\n",
            "Epoch[7/10](490/502) ||  training loss 0.01526 || training accuracy 91.25% || lr [9.990133642141917e-05]\n",
            "Epoch[7/10](500/502) ||  training loss 0.01568 || training accuracy 90.00% || lr [8.852566213879464e-05]\n",
            "Validation[8/10] ||  validation loss 0.04689 || validation accuracy 75.08%\n",
            "Epoch[8/10](10/502) ||  training loss 0.01639 || training accuracy 90.00% || lr [5.62666616782183e-05]\n",
            "Epoch[8/10](20/502) ||  training loss 0.01583 || training accuracy 90.62% || lr [2.591231629491673e-05]\n",
            "Epoch[8/10](30/502) ||  training loss 0.01485 || training accuracy 91.88% || lr [4.7586473766997275e-06]\n",
            "Epoch[8/10](40/502) ||  training loss 0.01396 || training accuracy 90.00% || lr [8.856374635655253e-07]\n",
            "Epoch[8/10](50/502) ||  training loss 0.01492 || training accuracy 90.62% || lr [1.5772644703562414e-05]\n",
            "Epoch[8/10](60/502) ||  training loss 0.01178 || training accuracy 91.88% || lr [4.37333383217769e-05]\n",
            "Epoch[8/10](70/502) ||  training loss 0.01136 || training accuracy 93.75% || lr [7.408768370507296e-05]\n",
            "Epoch[8/10](80/502) ||  training loss 0.009628 || training accuracy 93.75% || lr [9.52413526232842e-05]\n",
            "Epoch[8/10](90/502) ||  training loss 0.01597 || training accuracy 90.00% || lr [9.911436253641762e-05]\n",
            "Epoch[8/10](100/502) ||  training loss 0.01034 || training accuracy 92.50% || lr [8.422735529641999e-05]\n",
            "Epoch[8/10](110/502) ||  training loss 0.01402 || training accuracy 92.50% || lr [5.626666167820529e-05]\n",
            "Epoch[8/10](120/502) ||  training loss 0.01419 || training accuracy 92.50% || lr [2.5912316294910594e-05]\n",
            "Epoch[8/10](130/502) ||  training loss 0.01273 || training accuracy 91.25% || lr [4.7586473766985036e-06]\n",
            "Epoch[8/10](140/502) ||  training loss 0.01289 || training accuracy 94.38% || lr [8.856374635656516e-07]\n",
            "Epoch[8/10](150/502) ||  training loss 0.01699 || training accuracy 89.38% || lr [1.577264470357332e-05]\n",
            "Epoch[8/10](160/502) ||  training loss 0.01206 || training accuracy 91.88% || lr [4.3733338321806805e-05]\n",
            "Epoch[8/10](170/502) ||  training loss 0.01049 || training accuracy 93.12% || lr [7.408768370512333e-05]\n",
            "Epoch[8/10](180/502) ||  training loss 0.0135 || training accuracy 91.25% || lr [9.52413526233487e-05]\n",
            "Epoch[8/10](190/502) ||  training loss 0.01433 || training accuracy 92.50% || lr [9.911436253648484e-05]\n",
            "Epoch[8/10](200/502) ||  training loss 0.01516 || training accuracy 93.12% || lr [8.422735529647669e-05]\n",
            "Epoch[8/10](210/502) ||  training loss 0.0145 || training accuracy 91.88% || lr [5.6266661678244375e-05]\n",
            "Epoch[8/10](220/502) ||  training loss 0.02048 || training accuracy 89.38% || lr [2.5912316294927795e-05]\n",
            "Epoch[8/10](230/502) ||  training loss 0.01534 || training accuracy 91.88% || lr [4.758647376701574e-06]\n",
            "Epoch[8/10](240/502) ||  training loss 0.01809 || training accuracy 90.00% || lr [8.85637463565519e-07]\n",
            "Epoch[8/10](250/502) ||  training loss 0.01103 || training accuracy 92.50% || lr [1.577264470356898e-05]\n",
            "Epoch[8/10](260/502) ||  training loss 0.01124 || training accuracy 94.38% || lr [4.37333383217973e-05]\n",
            "Epoch[8/10](270/502) ||  training loss 0.01893 || training accuracy 88.75% || lr [7.408768370510696e-05]\n",
            "Epoch[8/10](280/502) ||  training loss 0.01029 || training accuracy 92.50% || lr [9.524135262332745e-05]\n",
            "Epoch[8/10](290/502) ||  training loss 0.02769 || training accuracy 88.12% || lr [9.911436253646253e-05]\n",
            "Epoch[8/10](300/502) ||  training loss 0.02041 || training accuracy 88.75% || lr [8.422735529645856e-05]\n",
            "Epoch[8/10](310/502) ||  training loss 0.02799 || training accuracy 83.75% || lr [5.626666167823142e-05]\n",
            "Epoch[8/10](320/502) ||  training loss 0.02077 || training accuracy 86.88% || lr [2.5912316294921683e-05]\n",
            "Epoch[8/10](330/502) ||  training loss 0.01618 || training accuracy 90.62% || lr [4.758647376700362e-06]\n",
            "Epoch[8/10](340/502) ||  training loss 0.01588 || training accuracy 89.38% || lr [8.856374635654359e-07]\n",
            "Epoch[8/10](350/502) ||  training loss 0.01746 || training accuracy 89.38% || lr [1.5772644703566565e-05]\n",
            "Epoch[8/10](360/502) ||  training loss 0.0152 || training accuracy 89.38% || lr [4.373333832179023e-05]\n",
            "Epoch[8/10](370/502) ||  training loss 0.01683 || training accuracy 91.25% || lr [7.408768370509472e-05]\n",
            "Epoch[8/10](380/502) ||  training loss 0.01321 || training accuracy 91.88% || lr [9.524135262331144e-05]\n",
            "Epoch[8/10](390/502) ||  training loss 0.02124 || training accuracy 88.12% || lr [9.911436253644563e-05]\n",
            "Epoch[8/10](400/502) ||  training loss 0.01475 || training accuracy 91.88% || lr [8.422735529644503e-05]\n",
            "Epoch[8/10](410/502) ||  training loss 0.01348 || training accuracy 90.62% || lr [5.626666167822293e-05]\n",
            "Epoch[8/10](420/502) ||  training loss 0.01343 || training accuracy 91.25% || lr [2.5912316294916984e-05]\n",
            "Epoch[8/10](430/502) ||  training loss 0.01403 || training accuracy 91.25% || lr [4.758647376699405e-06]\n",
            "Epoch[8/10](440/502) ||  training loss 0.01019 || training accuracy 95.00% || lr [8.856374635660949e-07]\n",
            "Epoch[8/10](450/502) ||  training loss 0.01473 || training accuracy 90.62% || lr [1.577264470357747e-05]\n",
            "Epoch[8/10](460/502) ||  training loss 0.01748 || training accuracy 91.25% || lr [4.373333832181871e-05]\n",
            "Epoch[8/10](470/502) ||  training loss 0.01649 || training accuracy 90.00% || lr [7.408768370514504e-05]\n",
            "Epoch[8/10](480/502) ||  training loss 0.01919 || training accuracy 90.00% || lr [9.52413526233759e-05]\n",
            "Epoch[8/10](490/502) ||  training loss 0.01717 || training accuracy 91.88% || lr [9.911436253651252e-05]\n",
            "Epoch[8/10](500/502) ||  training loss 0.01046 || training accuracy 95.00% || lr [8.422735529650168e-05]\n",
            "Validation[9/10] ||  validation loss 0.04906 || validation accuracy 75.69%\n",
            "Epoch[9/10](10/502) ||  training loss 0.01484 || training accuracy 91.25% || lr [5.000000000004013e-05]\n",
            "Epoch[9/10](20/502) ||  training loss 0.01527 || training accuracy 93.12% || lr [2.061073738539305e-05]\n",
            "Epoch[9/10](30/502) ||  training loss 0.009596 || training accuracy 94.38% || lr [2.447174185244373e-06]\n",
            "Epoch[9/10](40/502) ||  training loss 0.0116 || training accuracy 95.62% || lr [2.44717418524319e-06]\n",
            "Epoch[9/10](50/502) ||  training loss 0.01064 || training accuracy 95.00% || lr [2.0610737385387892e-05]\n",
            "Epoch[9/10](60/502) ||  training loss 0.009983 || training accuracy 95.00% || lr [5.000000000002683e-05]\n",
            "Epoch[9/10](70/502) ||  training loss 0.008673 || training accuracy 95.62% || lr [7.938926261466859e-05]\n",
            "Epoch[9/10](80/502) ||  training loss 0.0117 || training accuracy 94.38% || lr [9.755282581481228e-05]\n",
            "Epoch[9/10](90/502) ||  training loss 0.01268 || training accuracy 93.12% || lr [9.755282581481322e-05]\n",
            "Epoch[9/10](100/502) ||  training loss 0.01206 || training accuracy 91.88% || lr [7.938926261466878e-05]\n",
            "Epoch[9/10](110/502) ||  training loss 0.01424 || training accuracy 89.38% || lr [5.000000000002713e-05]\n",
            "Epoch[9/10](120/502) ||  training loss 0.01207 || training accuracy 92.50% || lr [2.0610737385388133e-05]\n",
            "Epoch[9/10](130/502) ||  training loss 0.01113 || training accuracy 93.75% || lr [2.4471741852441554e-06]\n",
            "Epoch[9/10](140/502) ||  training loss 0.01307 || training accuracy 91.88% || lr [2.447174185243182e-06]\n",
            "Epoch[9/10](150/502) ||  training loss 0.01292 || training accuracy 92.50% || lr [2.061073738538238e-05]\n",
            "Epoch[9/10](160/502) ||  training loss 0.01152 || training accuracy 94.38% || lr [5.000000000001732e-05]\n",
            "Epoch[9/10](170/502) ||  training loss 0.01012 || training accuracy 93.75% || lr [7.938926261464976e-05]\n",
            "Epoch[9/10](180/502) ||  training loss 0.01007 || training accuracy 93.12% || lr [9.755282581479077e-05]\n",
            "Epoch[9/10](190/502) ||  training loss 0.01131 || training accuracy 94.38% || lr [9.755282581479065e-05]\n",
            "Epoch[9/10](200/502) ||  training loss 0.01399 || training accuracy 91.25% || lr [7.93892626146517e-05]\n",
            "Epoch[9/10](210/502) ||  training loss 0.008108 || training accuracy 94.38% || lr [5.000000000001689e-05]\n",
            "Epoch[9/10](220/502) ||  training loss 0.01432 || training accuracy 92.50% || lr [2.0610737385384335e-05]\n",
            "Epoch[9/10](230/502) ||  training loss 0.009699 || training accuracy 95.00% || lr [2.4471741852430573e-06]\n",
            "Epoch[9/10](240/502) ||  training loss 0.0102 || training accuracy 93.12% || lr [2.447174185244502e-06]\n",
            "Epoch[9/10](250/502) ||  training loss 0.0127 || training accuracy 91.88% || lr [2.0610737385395424e-05]\n",
            "Epoch[9/10](260/502) ||  training loss 0.007723 || training accuracy 96.88% || lr [5.0000000000050004e-05]\n",
            "Epoch[9/10](270/502) ||  training loss 0.01494 || training accuracy 91.25% || lr [7.938926261470481e-05]\n",
            "Epoch[9/10](280/502) ||  training loss 0.0158 || training accuracy 90.00% || lr [9.755282581485636e-05]\n",
            "Epoch[9/10](290/502) ||  training loss 0.0202 || training accuracy 88.75% || lr [9.755282581485692e-05]\n",
            "Epoch[9/10](300/502) ||  training loss 0.01694 || training accuracy 90.62% || lr [7.938926261470396e-05]\n",
            "Epoch[9/10](310/502) ||  training loss 0.01845 || training accuracy 88.12% || lr [5.000000000005178e-05]\n",
            "Epoch[9/10](320/502) ||  training loss 0.01984 || training accuracy 89.38% || lr [2.0610737385396847e-05]\n",
            "Epoch[9/10](330/502) ||  training loss 0.01079 || training accuracy 93.75% || lr [2.4471741852450444e-06]\n",
            "Epoch[9/10](340/502) ||  training loss 0.01291 || training accuracy 91.25% || lr [2.447174185242292e-06]\n",
            "Epoch[9/10](350/502) ||  training loss 0.01072 || training accuracy 95.00% || lr [2.0610737385373652e-05]\n",
            "Epoch[9/10](360/502) ||  training loss 0.01699 || training accuracy 91.25% || lr [4.999999999999267e-05]\n",
            "Epoch[9/10](370/502) ||  training loss 0.01165 || training accuracy 92.50% || lr [7.93892626146146e-05]\n",
            "Epoch[9/10](380/502) ||  training loss 0.01166 || training accuracy 92.50% || lr [9.75528258147462e-05]\n",
            "Epoch[9/10](390/502) ||  training loss 0.01167 || training accuracy 93.12% || lr [9.755282581474653e-05]\n",
            "Epoch[9/10](400/502) ||  training loss 0.01324 || training accuracy 91.88% || lr [7.938926261461545e-05]\n",
            "Epoch[9/10](410/502) ||  training loss 0.009867 || training accuracy 95.62% || lr [4.999999999999657e-05]\n",
            "Epoch[9/10](420/502) ||  training loss 0.01148 || training accuracy 93.12% || lr [2.0610737385374516e-05]\n",
            "Epoch[9/10](430/502) ||  training loss 0.01603 || training accuracy 91.88% || lr [2.4471741852417465e-06]\n",
            "Epoch[9/10](440/502) ||  training loss 0.009223 || training accuracy 95.00% || lr [2.4471741852436112e-06]\n",
            "Epoch[9/10](450/502) ||  training loss 0.009816 || training accuracy 96.25% || lr [2.0610737385386696e-05]\n",
            "Epoch[9/10](460/502) ||  training loss 0.008951 || training accuracy 94.38% || lr [5.0000000000028185e-05]\n",
            "Epoch[9/10](470/502) ||  training loss 0.01018 || training accuracy 92.50% || lr [7.938926261466734e-05]\n",
            "Epoch[9/10](480/502) ||  training loss 0.0134 || training accuracy 90.00% || lr [9.755282581481264e-05]\n",
            "Epoch[9/10](490/502) ||  training loss 0.01975 || training accuracy 88.75% || lr [9.755282581481274e-05]\n",
            "Epoch[9/10](500/502) ||  training loss 0.01054 || training accuracy 95.62% || lr [7.938926261466764e-05]\n",
            "Validation[10/10] ||  validation loss 0.05011 || validation accuracy 73.23%\n",
            "Epoch[10/10](10/502) ||  training loss 0.01198 || training accuracy 93.12% || lr [4.373333832180951e-05]\n",
            "Epoch[10/10](20/502) ||  training loss 0.01364 || training accuracy 92.50% || lr [1.5772644703575445e-05]\n",
            "Epoch[10/10](30/502) ||  training loss 0.009982 || training accuracy 95.00% || lr [8.85637463566023e-07]\n",
            "Epoch[10/10](40/502) ||  training loss 0.01218 || training accuracy 91.88% || lr [4.7586473767047394e-06]\n",
            "Epoch[10/10](50/502) ||  training loss 0.009271 || training accuracy 94.38% || lr [2.5912316294945773e-05]\n",
            "Epoch[10/10](60/502) ||  training loss 0.01122 || training accuracy 93.75% || lr [5.6266661678287356e-05]\n",
            "Epoch[10/10](70/502) ||  training loss 0.007819 || training accuracy 95.62% || lr [8.42273552965441e-05]\n",
            "Epoch[10/10](80/502) ||  training loss 0.006575 || training accuracy 96.88% || lr [9.91143625365626e-05]\n",
            "Epoch[10/10](90/502) ||  training loss 0.01196 || training accuracy 92.50% || lr [9.524135262342485e-05]\n",
            "Epoch[10/10](100/502) ||  training loss 0.01021 || training accuracy 94.38% || lr [7.408768370518159e-05]\n",
            "Epoch[10/10](110/502) ||  training loss 0.009974 || training accuracy 93.12% || lr [4.373333832183975e-05]\n",
            "Epoch[10/10](120/502) ||  training loss 0.01715 || training accuracy 90.00% || lr [1.577264470358569e-05]\n",
            "Epoch[10/10](130/502) ||  training loss 0.007721 || training accuracy 95.62% || lr [8.856374635668758e-07]\n",
            "Epoch[10/10](140/502) ||  training loss 0.008341 || training accuracy 96.25% || lr [4.758647376698925e-06]\n",
            "Epoch[10/10](150/502) ||  training loss 0.008746 || training accuracy 95.62% || lr [2.5912316294918156e-05]\n",
            "Epoch[10/10](160/502) ||  training loss 0.008657 || training accuracy 96.25% || lr [5.626666167822295e-05]\n",
            "Epoch[10/10](170/502) ||  training loss 0.01032 || training accuracy 93.75% || lr [8.42273552964485e-05]\n",
            "Epoch[10/10](180/502) ||  training loss 0.01 || training accuracy 93.75% || lr [9.91143625364509e-05]\n",
            "Epoch[10/10](190/502) ||  training loss 0.01254 || training accuracy 92.50% || lr [9.524135262331696e-05]\n",
            "Epoch[10/10](200/502) ||  training loss 0.01121 || training accuracy 92.50% || lr [7.408768370509918e-05]\n",
            "Epoch[10/10](210/502) ||  training loss 0.01009 || training accuracy 93.75% || lr [4.373333832179444e-05]\n",
            "Epoch[10/10](220/502) ||  training loss 0.01051 || training accuracy 94.38% || lr [1.5772644703568736e-05]\n",
            "Epoch[10/10](230/502) ||  training loss 0.01802 || training accuracy 92.50% || lr [8.856374635655466e-07]\n",
            "Epoch[10/10](240/502) ||  training loss 0.008426 || training accuracy 96.25% || lr [4.758647376702888e-06]\n",
            "Epoch[10/10](250/502) ||  training loss 0.0076 || training accuracy 95.62% || lr [2.5912316294934714e-05]\n",
            "Epoch[10/10](260/502) ||  training loss 0.01019 || training accuracy 94.38% || lr [5.626666167826272e-05]\n",
            "Epoch[10/10](270/502) ||  training loss 0.007549 || training accuracy 94.38% || lr [8.422735529650462e-05]\n",
            "Epoch[10/10](280/502) ||  training loss 0.006677 || training accuracy 98.12% || lr [9.911436253651815e-05]\n",
            "Epoch[10/10](290/502) ||  training loss 0.01106 || training accuracy 93.12% || lr [9.524135262338173e-05]\n",
            "Epoch[10/10](300/502) ||  training loss 0.01607 || training accuracy 93.12% || lr [7.408768370514766e-05]\n",
            "Epoch[10/10](310/502) ||  training loss 0.006427 || training accuracy 97.50% || lr [4.373333832182222e-05]\n",
            "Epoch[10/10](320/502) ||  training loss 0.008253 || training accuracy 96.25% || lr [1.5772644703580178e-05]\n",
            "Epoch[10/10](330/502) ||  training loss 0.009493 || training accuracy 95.62% || lr [8.856374635663505e-07]\n",
            "Epoch[10/10](340/502) ||  training loss 0.01466 || training accuracy 91.88% || lr [4.758647376697077e-06]\n",
            "Epoch[10/10](350/502) ||  training loss 0.01253 || training accuracy 90.62% || lr [2.5912316294907094e-05]\n",
            "Epoch[10/10](360/502) ||  training loss 0.01116 || training accuracy 94.38% || lr [5.626666167819832e-05]\n",
            "Epoch[10/10](370/502) ||  training loss 0.007603 || training accuracy 96.25% || lr [8.42273552964091e-05]\n",
            "Epoch[10/10](380/502) ||  training loss 0.008713 || training accuracy 94.38% || lr [9.911436253640647e-05]\n",
            "Epoch[10/10](390/502) ||  training loss 0.009008 || training accuracy 93.12% || lr [9.524135262327385e-05]\n",
            "Epoch[10/10](400/502) ||  training loss 0.01169 || training accuracy 93.12% || lr [7.408768370506527e-05]\n",
            "Epoch[10/10](410/502) ||  training loss 0.01178 || training accuracy 93.75% || lr [4.3733338321774116e-05]\n",
            "Epoch[10/10](420/502) ||  training loss 0.01009 || training accuracy 95.62% || lr [1.577264470356115e-05]\n",
            "Epoch[10/10](430/502) ||  training loss 0.01031 || training accuracy 93.12% || lr [8.856374635655541e-07]\n",
            "Epoch[10/10](440/502) ||  training loss 0.0132 || training accuracy 91.25% || lr [4.7586473767010404e-06]\n",
            "Epoch[10/10](450/502) ||  training loss 0.008223 || training accuracy 94.38% || lr [2.5912316294923638e-05]\n",
            "Epoch[10/10](460/502) ||  training loss 0.01047 || training accuracy 93.12% || lr [5.626666167823804e-05]\n",
            "Epoch[10/10](470/502) ||  training loss 0.01361 || training accuracy 92.50% || lr [8.422735529646717e-05]\n",
            "Epoch[10/10](480/502) ||  training loss 0.009446 || training accuracy 94.38% || lr [9.911436253647313e-05]\n",
            "Epoch[10/10](490/502) ||  training loss 0.008509 || training accuracy 96.88% || lr [9.524135262333859e-05]\n",
            "Epoch[10/10](500/502) ||  training loss 0.008544 || training accuracy 95.62% || lr [7.408768370511624e-05]\n",
            "Validation[10/10] ||  validation loss 0.05401 || validation accuracy 71.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrqSafrzn7M1"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}